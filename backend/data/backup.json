[
  {
    "front": "Pre-Training vs Post-Training",
    "back": "In short:\n- Pre-training: learn general language features and patternrs\n- Post-training: tailoring the LLM to complete a specific task\n\n*Pre-training* (the part that usually entails only unsupervised learning) involves training the entire model architecture (encoder, decoder, or both) on a large corpus of unlabeled data to learn general language features and patterns. This process typically includes:\n- Random initialization of all model parameters\n- Training on vast amounts of diverse text data\n- Learning general language understanding and knowledge\n\n*Post-training* (the part that usually entails only supervised learning), which includes fine-tuning and other adaptation techniques, involves:\n- Loading parameters from the pre-trained model\n- Training on smaller, task-specific datasets\n- Adjusting model parameters to optimize performance for specific tasks/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nDoes ChatGPT use encoder-decoder architectures?",
    "back": "No, it uses decoder-only architecture\n\nGoogle Translate uses encoder-decoder architectures to translate text\n\nImage captioning services, text summarization services, and speech recognition services all use encoder-decoder architectures/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPrompting / In-Context Learning",
    "back": "Using prompt engineering to get a desired output from an LLM, rather than finetuning/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nImplementing Masked Language Modeling (MLM)",
    "back": "- Choose 15% of the words\n- Mask 80% of those words using a [MASK] token, change 10% of those words to something random, and leave the other 10% unchanged/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/n____ has been shown to hurt LLM training",
    "back": "Next Sentence Prediction\n\nsource: https://taoyds.github.io/assets/courses/COMP3361-lec11.pdf/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nExamples of encoder-only, decoder-only, and encoder-decoder architectures, and pros / cons of each",
    "back": "Encoder-only: BERT\n- Considers bi-directional context\n- Capture intricate contextual relationships\n- Fastest at making inferences since it doesn't generate sequences\n- Not good at generating open-text from left-to-right, one token at a time\n\nDecoder-only: GPT\n- Great at generative tasks\n- Doesn't learn bi-directional context or complex relationships\n- Slower than encoders at inference time since each word has to be predicted sequentially\n\nEncoder-decoder: T5\n- A nice middle ground between leveraging bidirectional contexts and open-text generation\n- Good for multi-task ne-tuning\n\n- Require more text wrangling\n- Harder to train\n- Less flexible for natural language generation\n- Slowest at making inferences/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nEvaluating LLM Performance",
    "back": "### 0. Automatic Metrics:\n- Perplexity: How well an LLM does at predicting the next word in a sequence\n- BLEU: Compare the model's output with some reference transcription\n- ROGUE: Measures similarity between summary and annotation.\n- Accuracy: Whether the model returns the expected output for a given question.\n\n### 1. Quality of Content \n- **Correctness:**: Assesses factual accuracy based on ground truth. \n- **Relevance:**: Evaluates how well the output addresses the given input. \n- **Hallucination:**: Measures the presence of fabricated or incorrect information. \n\n### 2. Linguistic Characteristics \n- **Coherence and Fluency:**: Evaluates grammatical correctness, readability, and logical consistency. \n- **Conciseness:**: Assesses how efficiently the information is conveyed. \n\n### 3. Cross-Cutting Considerations \n- **Quantitative vs. Qualitative:** - Balance between numerical scores and human judgment. \n- **Consistency:** - Assesses the model's ability to provide stable outputs across multiple runs. \n- **Safety and Responsibility:** \n- Evaluates for bias, toxicity, and ethical considerations./n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhy Transformers over RNNs?",
    "back": "- RNNs have trouble keeping track of relationships between words over large sequences (vanishing gradient problem)\n- RNNs aren't parallelizable because they have a feedback loop, whereas Transformers do not -> allows for training on significantly more data \n- The catch is that Transformers need positional encodings to understand the sequence of words, whereas RNNs learn the sequence through their loops\n\nhttps://princeton-nlp.github.io/cos484/lectures/lec13.pdf/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhy is multi-head attention important?",
    "back": "- Having multiple attention functions (i.e., multiple heads) allows the network to capture different types of relationships. One head can focus on syntactic relationships, while the other can focus on semantic meanings, during the encoding process\n- Used in encoders, whereas masked multi-head attention is used in decoders/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat is masked multi-head attention?",
    "back": "- Goal: enable parallelizable while NOT looking into the future\n- We apply a mask over the self-attention scores of all words ahead of the target word so that we don't consider relationships with words that haven't yet been written, effectively preventing the model from \"cheating\" by looking at future tokens. This is essential for autoregressive tasks where predictions are made sequentially, such as text generation or translation\n- We use masked multi-head attention in the decoder/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPros and cons of transformers",
    "back": "- Easier to capture long-range dependencies: we draw attention between every pair of words! \u2022\n- Easier to parallelize: Q = XWQ K = XW K V = XWV \n\n- Are positional encodings enough to capture positional information? Otherwise self-attention is an unordered function of its input\n- Quadratic computation in self-attention can become very slow when the sequence length is large; that's why GPT sets a max length of 1024 tokens/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nExplain Best Match 25",
    "back": "- BM25 (Best Match 25) is a popular ranking function used in information retrieval systems to estimate the relevance of documents to a given search query. \n- It improves upon the traditional TF-IDF method by addressing issues such as keyword saturation and document length normalization\n- The BM25 formula calculates a score for each document relative to a specific query, with higher scores indicating greater relevance. The score is similar to TF-IDF, except longer documents are penalized to avoid having a higher score\nexample bm25 index:\nbm25_index = { \"hello\": \n { \"doc_ids\": [1], \n \"term_frequencies\": {1: 1}, \n \"idf\": 0.693, \n\"doc_lengths\": {1: 4} }, .../n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nFull hybrid search",
    "back": "Finding the most relevant docs for a query using both BM25 and vector search, then merging and re-ranking the results\n\n1. It performs a vector search using a distance metric (typically cosine or dot product).\n2. It performs a full-text search using the BM25 scoring algorithm.\n3. It merges the results using Reciprocal Rank Fusion algorithm, which uses the summed reciprocal rank of every document across both searches to determine the top k docs.\n4. It re-ranks the results using semantic ranker, a machine learning model that compares each result to the original usery query and assigns a score from 0-4./n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nQuantization",
    "back": "- representing model parameters with lower precision (8-bit vs. 32-bit) to reduce memory footprint and increase inference speed\n- typically done after training to speed-up the inference process/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nSpectogram",
    "back": "A spectrogram is a visual representation of the spectrum of frequencies in a sound or other signal as they vary with time. It's usually depicted as a heat map or image where:\n- The horizontal axis represents time\n- The vertical axis represents frequency\n- The color or brightness intensity indicates the amplitude or energy of the frequency at each point in time\n\nFor example, in a spectrogram of speech:\n- You might see horizontal lines or bands representing sustained vowel sounds\n- Vertical lines or streaks could indicate sudden consonant sounds\n- The overall pattern would change as different words are spoken, with varying intensities across different frequency ranges\n\nYou can usually store spectograms to avoid privacy considerations that would prevent you from storing actual speech/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nIs passing user context into a prompt an example of RAG?",
    "back": "No. RAG really should involve querying for information that's useful for the user's specific request. The documents retrieved should be from authoritative, external sources, and should change depending on what the user asked.\n\nQuery matching -> looking up the correct output based on similar, annotated queries that are stored in vector search\n\nAn example of RAG -> retrieving relevant conversation history from prior conversations/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat metrics are used for measuring NLP outputs?",
    "back": "- Word Error Rate: The % of words that were correct. Usually used for ASR\n- BLEU measures how many n-grams (like words or sequences of words) in the candidate translation appear in the reference translation, focusing on precision. This is important in translation tasks where generating correct phrases and word order is essential for fluency and accuracy.\n- ROUGE evaluates the overlap of n-grams between the generated summary and the reference. It emphasizes recall, making it suitable for summarization tasks where capturing key points from the original text is critical./n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat's the difference between MIPS (Maximum Inner Product Search) vs cosine similarity?",
    "back": "- MIPS: This method is sensitive to the magnitude of the vectors, meaning that larger vectors can disproportionately influence the results. This is helpful when document length is a positive signal of relevance.\n- Cosine similarity: measures the angle between two vectors and is normalized by their magnitudes. This means it evaluates similarity based solely on direction rather than magnitude, making it useful in contexts where the length of the vector should not affect similarity scores\n\nNote that the original RAG paper used MIPS./n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nIn-Context Learning",
    "back": "Instead of fine-tuning (which requires calculating the gradient and updating parameters), we pass the following into the prompt itself:\n- Task descriptions\n- Input-output examples (demonstrations)\n- The specific query to be answered\n\nThe more examples we provide within the context window, the better the performance/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nHow does Reinforcement Learning with Human Feedback (RLHF) work?",
    "back": "- we train a separate reward model using human feedback data, where humans rank which LLM output they like better\n- the reward model is used to rank LLM outputs and return a relevance score\n- we adjust the model weights such that we maximize the odds of receiving a high relevance score/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat does \"parameters\" mean in the context of LLMs?",
    "back": "- Weights: Weights are **numerical values that define the strength of connections between neurons across different layers in the model**. In the context of LLMs, weights are primarily used in the attention mechanism and the feedforward neural networks that make up the model's architecture.\n- Biases: Biases are additional numerical values that are **added to the weighted sum of inputs before being passed through an activation function for each neuron in FF neural network layers and the multi-head attention layers**. They help to control the output of neurons and provide flexibility in the model's learning process. Biases can be thought of as a way to shift the activation function to the left or right, allowing the model to learn more complex patterns and relationships in the input data\n\nBoth weights and biases start as random coefficients which are adjusted during training to minimize the loss function/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/n__ layers often use GeLU / ReLU for their activation functions, while __ layers usually use softmax",
    "back": "FF neural networks; attention mechanisms/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nFF neural networks are fully connected, which means:",
    "back": "Each neuron in a layer is connected to all other neurons in the previous layer/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat is the output layer in an LLM?",
    "back": "- A FF NN that generates the final prediction of the network\n- Uses a softmax activation function for outputting probabilities, or ReLU / GeLU for classification tasks\n- Output is the probability of producing each word or token/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPost-training techniques to improve LLM outputs (7)",
    "back": "- In-Context Learning: Providing examples for your desired output\n- RAG: Querying a database to provide additional context to the LLM\n- Query Matching: Providing examples of prior queries to guide the LLM\n- Chain of Thought Reasoning: Asking the LLM to explain how it arrived at a result\n- RLHF: Using a seperate reward model to tune the LLM's parameters to better match human preferences\n- Direct Preference Optimization: An alternative to RLHF where we optimize the model weights directly by maximizing the odds that the LLM returns the preferred output without training a separate reward model\n- Quantization: Compressing model parameters to int16 or int8/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nProximal Policy Optimization (PPO)",
    "back": "- a reinforcement learning algorithm developed in 2017\n- samples actions according to the latest version of its stochastic policies\n- clips the probability ratio of new vs. old policies to avoid making too big of changes to its policies per turn\n- uses only the first-order derivative (the gradient) to be more efficient vs. other optimizations/n",
    "tags": [
      "llms"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Highest-level framework for solving cases",
    "back": "- Mission\n- KPIs\n- Constraints\n- Strategy\n- Summary",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Complete set of frameworks, in prioritized order",
    "back": "Customer, Product, Company, External, Implementation (Design, Hardware, Software)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Customer Analysis",
    "back": "- *Who* will we target? (Segmentation)\n- What *behaviors* do these personas exhibit (customer value chain)? Which are value-creating and which are value-destroying.\n- What *needs* will we solve (Targeting)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Disintermediation",
    "back": "cutting-out the middle man to improve customer experience",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Product Analysis*",
    "back": "- *Value proposition* for our persona\n- *Features* we could build to address specific needs\n- *Roadmap* to prioritize features\n- *KPIs* to track our progress as we go\n- *Profitability* (upfront costs, profitability, synergies)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Template for user stories",
    "back": "\"As a __, I want to __ when I __ so I can __\"",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "When identifying features...",
    "back": "... consider the customer value chain, then identify needs, then turn them into features",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four criteria for prioritizing features on a roadmap",
    "back": "- Customer impact (frequency x benefit)\n- Economics (revenue / costs)\n- Complexity / Confidence Probability\n- Intangibles",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Framework for thinking about sales and KPIs",
    "back": "- Reach - make them aware\n- Engage - get in their consideration set\n- Monetize- get them to make a purchase\n- Nurture - build a long-term relationship",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 \"Reach\" KPIs",
    "back": "- Cost per impression\n- Number of impressions\n- Audience volume\n- Audience quality",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 \"Engage\" KPIs (6)",
    "back": "- Downloads / Trials\n- Traffic\n- Users / Accounts\n- Frequency\n- Time\n- Interactions (Clicks \\ likes / Shares / items in cart)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 \"Monetize\" KPIs",
    "back": "- $ / per user\n- CLV\n- CAC\n- Cost per click\n- Cart abandonment\n- Conversion\n- Purchase frequency\n- Basket size",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "5 \"Nurture\" KPIs",
    "back": "- Churn rate\n- Repeat purchases\n- Brand satisfaction\n- Promoter scores\n- Referral rates",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Company Analysis",
    "back": "- Mission\n- Differentiators\n- Products (others, lifecycles)\n- Customers\n- Capabilities",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Subcategories of a company's capabilities",
    "back": "Functional (R&D, Manufacturing, S&M, etc.)\n\nEnabling (People, Process, Technology, Org Structure)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four steps to an economic analysis",
    "back": "- Upfront cost / capital\n- Profitability\n- Positive Synergies\n- Negative Synergies",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Revenue subcategories",
    "back": "- Pricing\n- Market size\n- Market growth rate",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of variable costs (5)",
    "back": "-COGS\n- Labor\n- Materials \n- Repairs\n- Shipping",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of fixed costs (6)",
    "back": "- Salaries\n- Overhead\n- Insurance\n- Capital\n- Rent\n- Storage",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Two types of positive synergies",
    "back": "- Revenue synergies\n- Economies of scale",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four types of positive revenue synergies",
    "back": "- Increase your market size (new occassions along the value chain, channels, foot-traffic)\n\n- Make your existing customers purchase more (cross-sell, up-sell, frequency, basket size, bundling)\n\n- Increase prices\n\n- Reduce churn (loyalty programs, customer service, contracts)",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Negative synergies (3)",
    "back": "- Canabalization\n- Capacity constraints\n- Cultural degradation",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Differentiators / barriers to entry (7)",
    "back": "\u00a1Capital / Infrastructure\n\u00a1Regulations / Tariffs\n\u00a1Information / Data\n\u00a1Contracts / Relationships with Distributors / Suppliers\n\u00a1Economies of Scale / cost\n\u00a1Tech / IP\n\u00a1Switching costs to buyers",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Environmental analysis (6)",
    "back": "- Competitors\n- Complementors\n- Channels\n- Suppliers\n- Substitutes\n- External",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "External risks (5)",
    "back": "- Crime / Piracy / Counterfeiting / Privacy / Scamming / Data Loss\n- Regulatory / Patents / Certs\n- Insurance / Financing\n- Economic\n- Trends (Demographic, Cultural, Industry",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Structure to your conclusion",
    "back": "- Answer first\n- Who it's for\n- How it works\n- Why it's valuable to users\n- Risks / Trade-Offs\n- Next Steps",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pricing tactics",
    "back": "- Segmentation: charging particular needs more than others\n- Entry pricing: start low, then gradually increase\n- Bundling\n- Skimming: start higher, then lower it\n- Loss-leading: charging a lower initial price so you can earn subscription revenue",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 ways to evalute prices (3)",
    "back": "- Value-Based \n- Cost-Based\n- Competitor-Based",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Value-Based Pricing",
    "back": "setting price based on buyers' perceptions of value (their WTP) rather than on the seller's cost",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four easy factors to size a market (Fermi estimate)",
    "back": "- # of customers\n- Share of market\n- # of purchases per customer per year\n- Average revenue per purchase",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Analyzing competition (3)",
    "back": "- Barriers to entry\n- Fragmentation\n- Resources",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 considerations for how to build something",
    "back": "- Design principles\n- Hardware\n- Software",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Hardware options",
    "back": "- Desktop\n- Mobile\n- Tablet\n- Beacon\n- Integrated camera",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ways users can interact with a product",
    "back": "Think five senses",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Six design principles",
    "back": "- Innovative\n- Useful\n- Aesthetic\n- Unobtrusive\n- Honest\n- Understandable",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Types of 'fit' to evaluate",
    "back": "Product / Market\nProduct / Company",
    "tags": [
      "product_management"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Double-blind study",
    "back": "Where both the experimenter and experimentees don't know who's in the treatment group during the experiment",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "ANOVA",
    "back": "In a one-way ANOVA, the one factor or independent variable analyzed has three or more categorical groups. A two-way ANOVA instead compares multiple groups of two factors (e.g., is there a significant main effect for packaging and advertising, and is there an interaction effect between them?)\n\n- Commonly used in A/B testing\n- Null hypothesis: means of all groups are equal\n- Alt hypothesis: the means are not all equal (e.g., price effect exists)\n\nUnderlying assumptions: \n- Dependent variable is measures on interval or ratio scale\n- Normal distribution of the population from which the samples are drawn\n\nBreaks sum-of-squares down from two perspectives: \n- Between \n- Within \n- Sum is used to get F-statistics",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Stochastic vs. Deterministic",
    "back": "- Deterministic: you always get the same outputs from a set of inputs\n- Stochastic: there's randomness involved with the selection of your output",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Robustness",
    "back": "Checking that your outputs still make sense when you change your inputs (e.g., A/A testing)\n\nrobustness of a machine learning model refers to the stability of the model performance after adding some noise to the input data.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Loss function",
    "back": "Function used to assess goodness of fit relative to a target variable, taking the output of your model (e.g., linear formula y = bx + a) as a parameter in the form C(yi - (bx+a)). Typically used in optimization by finding the min of the loss function. Sometimes synonymous with objective function, though the latter is more general",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four things needed for machine learning",
    "back": "\u25cb Data\n \u25cb Model to transform the data (i.e., parameters)\n \u25cb Loss function to measure model performance\n \u25cb Algorithm to tweak the parameters such that the loss function is minimized",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Explain PCA",
    "back": "PCA is an orthogonal linear transformation that transforms some set of features into the n principal components that maximize explained variance in your dependent variable. The first vector is a weighting of various features such that explained variance is maximized, the second vector contains the second-most explained variance, etc. All of the vectors have a covariance of zero (e.g., are orthogonal to one another).\n\nPCA requires normalization",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Directed graphic models",
    "back": "discovering relationships between fields",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Markov Process",
    "back": "when the current state depends on previous states",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Contextual bandit problem",
    "back": "extension of the multi-armed bandit, except we make our choices depend on the context in which they're chosen (some exogenous variable)\n\ne.g., what products should we show on our home page given we know this user has bought a camera in the past?",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Multi-armed bandit problem",
    "back": "when there is no state, just a set of available actions with initially unknown rewards (e.g., determining which products to show on your homepage)\n\n- learning algorithm should balance exploration with exploitation, where you explore more in the beginning and exploit more when you have information on what to show to your users\n- used to avoid serving sub-optimal options to users more than necessary\n- uses an algorithm like Upper Confidence Bound1 to solve the problem",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Broadcasting (in the linear algebra context)",
    "back": "performing some operation (e.g., addition) from a vector to a matrix such that the vector is duplicated along any axis with dimension 1 to match the shape of the high-dimensional array",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Broadcasting (in the distributed computing context)",
    "back": "sending a small array to each of your worker nodes to hold in-memory",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Dot product",
    "back": "multiplying two arrays, then adding their elements",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Conditional independence",
    "back": "we can only say that two variables are independent if we witness the presence of a third variable",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Na\u00efve Bayes classification",
    "back": "Bayesian models that assume each feature (defined with a prior) is independent of other features\n\nscales much better than other Bayesian models",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Conjugate distributions",
    "back": "distributions that are made up of other distributions\n \u25cb Beta, Dirichlet, Gamma, and Wishart are made up of the Binomial, Multinomial, Poisson, and Gaussian distributions",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Autograd",
    "back": "Autograd is an automatic differentiation library widely used in machine learning, particularly in frameworks like PyTorch, to compute gradients efficiently. It enables backpropagation, which is the foundation of training neural networks.\n\nautomatically calculates derivatives every time you make a pass through your model by building a symbolic graph on the fly which can immediately backpropagate gradients",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Optimization challenges (4)",
    "back": "\u25cb Local minima\n \u25cb Saddle points (where the gradient is zero)\n \u25cb Analytical solutions vs. approximations\n \u25cb Machine precision",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Innovation state space models (ISSMs)",
    "back": "maintain a state vector of level, trend, seasonality, and a small Gaussian noise factor such that future occurrences are a sum of these factors",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Chi-Square Tests (3)",
    "back": "think \"categorical correlation\" using count data\n\nGoodness-of-Fit test for a One-Way Table\n- Determines whether a set of categorical data comes from a claimed distribution\n- The null hypothesis is that the proportion in each category in the population has a specific distribution. \n\nTest of Independence for a Two-Way Table\n- The null hypothesis says the two variables are independent (or not associated). The alternative hypothesis says the two variables are dependent (or associated).\n\nTest of Homogeneity for a Two-Way Table\n- The null hypothesis says that the distribution of proportions for all categories is the same in each group or population\n- E.g., did a button drive purchases?",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Z-test",
    "back": "used to test probability of two random variables being drawn from the same normal distribution",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "When to use t-tests",
    "back": "One independent cont. or categorical var (two levels), one dependent \n\nuse t-tests for small sample sizes (<50) or when the population variance is unknown",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Fourier transforms",
    "back": "A way to transform a single wave pattern into multiple sine waves http://www.jezzamon.com/fourier/index.html",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Type 1 vs. Type 2 Error",
    "back": "Type 1 - Rejection of a true null hypothesis (false positive)\n\nType 2 - Acceptance of a false null hypothesis (false negative)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "L1 vs. L2 regularization",
    "back": "L1 (Lasso): Adds absolute value of magnitude. *Can drive coefficients down to zero, effectively reducing the number of features considered*\n\nL2 (Ridge): Squared error lambda. Does *not* drive coefficients to zero.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Generative vs. discriminative",
    "back": "Generative models learn the distribution of each class\n\nDiscriminative models learn the boundaries between classes",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "F1 Score",
    "back": "Weighted average of precision and recall\n\nF1 Score = 2*(Recall * Precision) / (Recall + Precision)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Precision",
    "back": "Of the items we selected, how many were relevant?\n\nTP / (TP + FP)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Recall",
    "back": "Of the relevant items, how many did we select?\n\nTP / (TP + FN)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "When to use Z-tests vs. t-tests vs. chi-squared vs. Fischer's vs. ANOVA",
    "back": "When comparing quantitative metrics between two groups (e.g., the means of two groups)...\n- z-test: large sample size or known population variance\n- t-test: not a z-test\n\nWhen comparing quantitative metrics between three or more groups (e.g., the means of three or more groups)...\n- use ANOVA\n\nWhen comparing qualitative observations to see if some target variable changes given some occurence:\n- Chi-squared: large sample sizes\n- Fischer's test: smaller sample sizes",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Definition of a p-value",
    "back": "the probability of observing the observed data/results or something more extreme given that the null hypothesis is true\n\n- Z-test: probability that two groups have their means given that they come from the same distribution",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bayes Theorem",
    "back": "P(A|B) = P(A + B) / P(B) = P(B|A) * P(A) / P(B) = P(B|A) * P(A) / (P(B|A) * P(A) + P(B | not A) * P(not A))",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of ML errors that you want to trade-off",
    "back": "- Bias\n- Variance\n- Noise (irreducible)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bootstrapping",
    "back": "Drawing random samples with replacement",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Six steps for boosting",
    "back": "1) Generate one training set by random sampling with replacement, according to some sample weights (initialized with uniform weights)\n2) Fit one estimator using that training set\n3) If the single estimator achieves an accuracy greater than the acceptance threshold (e.g., 50% in a binary classifier, so that it performs better than chance), the estimator is kept, otherwise it is discarded. \n4) Give more weight to misclassified observations, and less weight to correctly classified observations, but trying to minimize residual errors from one iteration to the next.\n5) Repeat the previous steps until N estimators are produced. \n6) The ensemble forecast is the weighted average of the individual forecasts from the N models, where the weights are determined by the accuracy of the individual estimators",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "The trouble with recursive modeling",
    "back": "Your errors accumulate over time",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Central Limit Theorem (CLT)",
    "back": "distributions look more and more normal as sample sizes increase, even if the underlying distribution isn't normal",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Entropy",
    "back": "\u25cb An information-theoretic measure of the uncertainty contained in our dataset\n\u25cb Splitting a dataset on any attribute with multiple classes is guaranteed to lower the weighted average entropy between the resultant datasets\n\u25cb Attribute selection using entropy: split on the attribute that gives the greatest reduction in entropy (MOST INFORMATION GAIN)\n\u25cb It is measured in terms of bits of information, (think computer bits), where the entropy of a set of M distinct values is the number of bits needed to encode the values in the most efficient way\n\u25cb As we add new variables that capture information, we expect our entropy to decrease",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Jacobian",
    "back": "\u25cb A matrix of first-order partial derivatives \n\u25cb Use this matrix to determine which way to adjust parameters in gradient descent",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Hessian",
    "back": "A matrix of all second-order partial derivatives of a function\nUseful for understanding the curvature of a function",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why we A/B test and six-step process",
    "back": "have a scientific process for incrementally to ensure good ideas don't die young\n\nProcess for A/B testing (6) -> \n\u25cb Define your hypothesis statement\n\u25cb Define your five metrics (Overall Evaluation Criterion, independents, dependents, power, and significance level)\n\u25cb Gather a representative sample of your population\n\u25cb Split your test group into control and treatment groups such that your independent metrics are balanced\n\u25cb Run your experiment and ensure that your independent variables remained constant (sign of good experimental design)\n\u25cb Reject or fail to reject the null when comparing control group to your treatment group",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Power",
    "back": "\u25cb The probability that an experiment will flag a real change as statistically significant (the probability of rejecting the null given the null is false)\n\u25cb Also known as 1 - beta, or 1 - the type II error rate \n\u25cb Conventional power benchmark is 80%",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Power is a function of",
    "back": "\u25a1 The statistical significance required (more significance, less power)\n\u25a1 The magnitude of the effect of interest on the population (larger magnitude with smaller variance, more power)\n\u25a1 The sample size used to detect the effect (larger size, more power)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Standard error",
    "back": "the expectation of variance given some standard deviation of a sample population, where the assumed standard deviation of the population is divided by the root of the sample size\n\nSE = (ST. Dev) / root(n)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Significance level",
    "back": "\u25cb The probability of rejecting the null hypothesis when it is true\n\u25cb Also known as alpha, or the type 1 error rate",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "P-value",
    "back": "the probability of observing the observed data/results or something more extreme given that the null hypothesis is true",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Null hypothesis",
    "back": "the hypothesis that the treatment has no effect",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Best way to measure statistical variance in A/B testing",
    "back": "bootstrapping",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Sensitivity vs. robustness",
    "back": "Sensitivity = Does your dependent change when it should?\n\nRobustness = Does your dependent hold constant when it should?\n\nmetrics should be sensitive enough to be reliable indicators of change, but robust enough that they don't change often during the normal course of business",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why you shouldn't use A/A/B tests",
    "back": "You're better off pooling the control groups and exploring for variations within than doing outright A/A/B tests since they reduce your testing power and make it possible to reject one null and fail to reject another",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Network (or higher order) effects",
    "back": "\u25a1 When changes to your treatment group spill over to your control groups\n\u25a1 Particularly a problem when you're testing features that control how users interact with one another\n\u25a1 Solution: \n \u25a1 Split by community using the Normalized Cuts algorithm\n \u25a1 Communities can be defined as:\n \u00ae Geographic areas, drawing boundaries that maximizes the number within-region connections relative to connections across regions\n \u00ae Interest intersections, groups with many within-group connections and relatively few between-group connections \n \u00ae Some combination of geographic and interest features\n \u25a1 Resultant test will have less power as you'll have to adjust for the fact that users weren't assigned independently",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Treatment Group Dilution",
    "back": "\u25a1 If not all users in your treatment group are affected by the change, your treatment group will be diluted\n\u25a1 If you try to hold constant whether or not the user interacted with the feature change, you may bias your sample\n\nSolution: create samples based on usage",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Cherry-picking",
    "back": "not defining your key success metrics upfront, then picking one of several hundred different metrics to validate your result after the experiment\n\nSolution: create your metric upfront",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Performance issues in testing",
    "back": "experiments can cause unmeasured issues, like browser performance issues, which impact the dependent variable\n\nSolution: check performance benchmarks between test and control groups",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Cross-experimental issues",
    "back": "Being in more than one experiment could impact your key metrics\n\nSolution: be sure your experiments won't collide or keep them separate",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Timing effects",
    "back": "\u25a1 Effects may vary over time due to seasonality, learning (takes people a while to pickup the new feature), or novelty effects (excess usage in the beginning)\n\nSolution: run your experiments on small groups of users for long periods of time, keeping your test and control groups constant over that period",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Focusing too much on p-values",
    "back": "given a large enough sample size, even a .000000001 difference between means will result in a rejection of the null. \n\nSolution: focus both on p-values and effect sizes (with error margins)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bucketing issues",
    "back": "as you automatically assign users to test and control groups in active experiments, check that your bucket sizes meet expectations",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ramp-ups",
    "back": "after you complete your A/B test and want to roll-out your change to users, you do so on a rolling basis to test for usage effects on key metrics",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Holdbacks",
    "back": "rolling out a feature to 99% of users to observe how they diverge from the 1% without the change",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "P(A|B) =",
    "back": "P (A and B) / P(B) = P(B|A) * P(A) / P(B) = P(B|A) * P(A) / (P(B|A) * P(A) + P(B| not A) * P(not a))",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Law of large numbers",
    "back": "As a sample size goes to infinity, its sample mean gets closer to the population mean",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Exponential distribution",
    "back": "\u25a1 The amount of time until some event occurs given the event is memoryless\n\u25a1 Mean = 1 / lambda\n\u25a1 Variance = 1 / (lambda^2)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Gamma Distribution",
    "back": "\u25a1 Similar to exponential: the amount of time until k events occur, where each event is exponentially distributed\n\u25a1 Mean = (number of events) * (mean value of each event)\nVariance = (number of events) * (mean value of each event)^2",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Geometric",
    "back": "\u25a1 Used to model the number of failures before the first success\nMean = 1/probability of success\nVariance = (1-p)/(p^2)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Poisson",
    "back": "Used to model the number of rare events in some unit of time\nMean = lambda = successes * time since release\nVariance = lambda",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Uniform",
    "back": "\u25a1 Used to model events that are equally likely\n\u25a1 Mean = .5 * (a + b)\nVariance = 1/12 * (b-a)^2",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Binomial",
    "back": "\u25a1 When you have a probability of success and want to count the number of successes\n\u25a1 Expected value = n*p\n\u25a1 Variance = n*p * (1-p)\n\u25a1 St. Error = sqrt(p(1-p)/n)\n\u25a1 cdf = Combin(n, s)*p^s * (1-p)^(n-s)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Expected value of discrete distributions",
    "back": "sum(probability * outcome)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Expected variance of discrete distributions",
    "back": "E[X^2] - (E[X])^2",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Combination use and formula",
    "back": "use when order doesn't matter: n! / (n - r)!, where r is the number of successes in a set of n trials",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Permutation use and formula",
    "back": "use when order matters: n! / ((n - r)! * r!), where r is the number of successes in a set of n trials",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why and how to center a variable prior to regression",
    "back": "Subtract the mean from the array\n\nUse when it wouldn't make sense for a variable to equal zero (e.g., height), so you can interpret the intercept as \"the incremental effect of increasing your mean by one unit\"",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to think about correlation",
    "back": "- it's like regression of two variables, x and y, where x and y have been standardized\n- doesn't make much sense for variables that have already been rescaled (e.g., elasticities)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Find the confidence 99% confidence interval of a binomial with X=300 and n=2000",
    "back": ".129 - .171 (https://classroom.udacity.com/courses/ud257/lessons/4018018619/concepts/40043986940923)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Would you launch if:\n\nX control = 974\nN control = 10072\n\nX experiment = 1242\nN exp = 9886\n\ndmin = .02\nconfidence interval = 95%",
    "back": "",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Alternative to using distributions to measure 95% confidence intervals (3)",
    "back": "Use empirical methods:\n\n- Bootstrap n experiments (e.g., n=100)\n- Sort the different results by magnitude\n- Select the highest and lowest values immediately excluding the 95% of values in the middle",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bonferroni correction",
    "back": "Adjusts your statistical significance level by the number of metrics you're testing when evaluating multiple hypotheses (alpha / number of metrics)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Power",
    "back": "probability of correctly rejecting the null hypothesis (H0H_0H0\u200b) when the alternative hypothesis (H1H_1H1\u200b) is true. It measures the test's ability to detect an actual effect or difference when one exists.\n\nFactors Affecting Power:\n- Sample Size: Larger sample sizes increase power.\n- Effect Size: Larger effects are easier to detect, increasing power.\n- Significance Level (\u03b1\\alpha\u03b1): Higher \u03b1\\alpha\u03b1 increases power but also increases the risk of Type I errors.\n- Variability: Lower variability in the data increases power.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "5 issues to watch for when A/B testing:",
    "back": "CPCNT\n- Cross-experimental effects\n- Performance effects\n- Control group changes (A/A testing)\n- Network effects\n- Timing effects",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Berkson's paradox",
    "back": "Using statistics to draw incorrect inclusions about the general population when your sample was filtered on some condition",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Assume there's a 4% chance of being shown an ad on FB per view. What are the odds that you are shown exactly one ad",
    "back": "Binomial distribution with n = 100, r = 1, and p = .04\n\nProbability of getting 1 ad in 100: (.96)^99*(.04)\n\nDifferent ways you could get that probability (order doesn't matter): 100!/(1! * (99!)) = 100\n\nAnswer = 100 * (.96)^99 * .04 = 7%",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pareto",
    "back": "Continuous, used to plot the probability of an event happening over time",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "___ are to continuous distributions as ___ are to discrete distributions",
    "back": "probability density functions; probability mass functions",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Technique to remove noise from an image or outliers from a dataset",
    "back": "use autoencoders, which ignore outliers / unusual characteristics",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Locality-Sensitive Hashing",
    "back": "- Hashes values in such a way that near values are more likely to be placed near similar values in the lookup array\n- Can be used to do lookups and \"similar\" searches on very large datasets in a fast and efficient way\n- Great overview here: http://tylerneylon.com/a/lsh1/",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Multiples Testing",
    "back": "Testing many different hypotheses using a given dataset. to avoid having null hypotheses rejected due to luck when testing multiple hypotheses, you should use the false discovery rate method (Benjamin-Hochberg procedure):",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Problems with AUC",
    "back": "TLDR: Don't use it for imbalanced datasets\n\n- It ignores the actual values of scores and goodness of fit (you can have a strong AUC on an imbalanced dataset, but still experience poor out-of-sample performance)\n- It weighs false positives and false negatives equally",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Winsorization",
    "back": "- Replacing observations above the 99.9%th cut-off with the 99.9% cut-off\n- Reduces variance and your confidence interval by shrinking outliers",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ways to correct biases samples (2)",
    "back": "- Regression Adjustment: Given we have a biased cohort, adjust the mean of the test and control group using data from before the experiment began\n- Reshuffle: iteratively choose a randomization seed such that there is no bias in the metric of interest",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Back-test",
    "back": "- When you roll-out a new feature to 95% of users to look for differences with the 5% still using the original",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pre-test",
    "back": "- When you roll-out a new feature to 5% of new users to compare how their metrics change",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Confidence vs. predictive intervals",
    "back": "- Predictive intervals describe how much forward-looking predictions may vary\n- Confidence intervals describe how much a given parameter may vary",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Diff in diff",
    "back": "If you know your control and treatment group are going to change over time due to outside factors, you should measure the change in the control vs. the incremental lift provided by the treatment",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Batch vs. stochastic gradient descent",
    "back": "Stochastic descent involves calculating the gradient on a few, randomly-selected samples at a time\n\nBatch descent means using the whole dataset to calculate the gradient\n\nSGD is generally preferred for performance and optimization reasons",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "BLEU",
    "back": "A metric which compares a translated text to a reference text in order to calculate a penalized precision\n\nPrecision -> similar to 1-WER; can be calculated using 1-grams, 2-grams, 3-grams, 4-grams\n\nPrecision is penalized for brevity in BLEU to prevent the translation component from increasing their precision by predicting fewer words",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bayesian mixture model when you want to predict a count of things over time (e.g., # of people who will see a billboard)",
    "back": "Negative Binomial Distribution, mix of:\n\nPoisson: how many times an individual will do something given some probability lambda\nGamma: amount of time until k events given that probability",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bayesian mixture model when you want to predict how many of an item someone will purchase",
    "back": "Beta Binomial, mix of:\n\nBeta: probability of a purchase\nBinomial: # of purchases given that probability",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Beta distribution",
    "back": "Probability of an event happening (bounded between 0 and 1)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bayesian mixture model for finding \"given a bunch of products, which do they choose?\"",
    "back": "Dirichlet multinomial, mix of:\n\nMultinomial: probability of picking x\nDirichlet: preferences for each brand given the probabilities",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Bayesian mixture model for measuring retention / churn over some time period",
    "back": "Shifted beta geometric, which is a mix of:\n\n- Geometric: probability of churn\n- Beta: probability of churn at any given point in time",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What is R-squared, and why is it a bad idea to rely on it?",
    "back": "The percent of variance of the response variable that can be explained by the predictors\n\n- it's in-sample, so doesn't generalize well\n- doesn't measure 'goodness of fit'; can be arbitrarily close to 1 even when the model is completely wrong\n- does not measure how one variable explains another",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of scaling in ML and when to use them",
    "back": "Use scalers to:\n- control for outliers\n- visualize data more easily\n- make sure that all variables are on the same scale (necessary for Support Vector Machines)\n\nStandardScaler = (x-x_mean)/(x_standard_deviation) [mean of 0, st. dev. of 1]\nMinMaxScaler = (x - x_min)/x_max [data falls between 0 and 1]\nRobustScaler = (x-x_median)/x_quantile; moves all data to fall between 25th and 75th percentiles",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Metrics commonly used for unsupervised learning (clustering)",
    "back": "Silhouette Coefficient: Formula using a_) the mean distance between a sample and all other points in the same class and b) the mean distance between a sample and all other points in the next nearest cluster.\n\nThe score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\nThe score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Metrics commonly used for supervised learning",
    "back": "- Classification Metrics (accuracy, precision, recall, F1-score, ROC, AUC, ...)\n- Regression Metrics (MSE, MAE)\n- Ranking Metrics (MRR, DCG, NDCG)\n- NLP Metrics (Perplexity, BLEU score)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Metrics commonly used in NLP",
    "back": "- Perplexity: measurement of how well a probability distribution or probability model predicts a sample\n- BLEU score: a weighted score of how well a generated text matches an annotated result",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why is Python slow relative to C++?",
    "back": "- GIL means no concurrency\n- Interpretation happens at runtime\n- Dynamic typing means the CPython interpreter can't make optimizations under-the-hood",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "5 concepts for improving LLMs",
    "back": "- **Priming**: Passing-in additional context on the user to help influence the response\n- **Reasoning-traces**: Asking the LLM to explain its reasoning, or to answer the question one piece at a time, to improve performance\n- **Evaluation data**: Getting better evaluation sets to compare against using BLEU or ROUGE and catch errors as they happen\n- **Feedback**: Gathering feedback on responses to guide reinforcement learning.\n- **Fine-tuning**: Help the LLM perform well against specific tasks. Can fine-tune the pretraining, instructions, input data, or reinforcement learning with human feedback\n- **Guardrails** - Using regex or syntactic/semantic/safety checks to vet the LLM's response\n- **Retrieval-augmented generation (RAG)**: Using embeddings to retrieve relevant docs and passing them to the LLM to improve performance. Can use Google results, for example, to return more relevant answers.\n- **Chain-of-thought Prompting**: Not just asking the LLM for the answer, but asking it to output it's rationale, improves performance significantly (note: already built-in to most LLMs)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Zero Shot vs. One Shot vs. Few Shot",
    "back": "- Zero shot: there are no training examples available for a particular class during the transfer learning process\n- One shot: there is only example to train in during the transfer learning process\n- Few shot: there are only a few examples for each class to use when transfer learning",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Encoder models are trained via __, while decoder models are trained via __. __ models are a combination of encoder-decoder models.",
    "back": "Autoencoding (e.g., bidirectional masked text prediction). Autoregression (e.g., unidirectionally predicting what word comes next). Seq2seq",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four levels of anomaly detection",
    "back": "1. Point Anomalies: measure how many standard deviations a given point is from the mean\n\n2. Contextual Anomalies: Use XGBoost to create a predictive interval, then flag when we go outside of those bounds\n\n3. Trend Anomalies: Deconstruct into seasonality and trend components (e.g., using Seasonality Trend Decomposition), then flag when there is a changepoint in the trend\n\n4. Cluster Anomalies: Use unsupervised learning to group occurrences (e.g., DBSCAN, t-SNE, PCA, autoencoders, KNN), then measure how far a given occurrence is from the mean of its cluster group",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Easy way to get high precision, low recall\n\nEasy way to get high recall, low precision",
    "back": "Select rows where you're absolutely sure\n\nSelect everything",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How autoencoders work (3), and three use cases",
    "back": "- Start with a high-dimensional dataset (e.g., an image)\n- Add \"noise\" to the dataset\n- Reduce to a low-dimensional space (e.g., 10 features)\n- Use only those n features to reconstruct the original image wihtout noise \n\n- Dimensionality reduction\n- Inferring semantic meaning from the features\n- Easily identifying exceptions (e.g., if the dataset was trained on dogs, a cat's output would trigger an anomaly)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Clustering vs. dimensionality reduction, and examples of each",
    "back": "Clustering: group rows together (e.g., autoencoders, DBSCAN)\n\nDimensionality reduction: group columns together (e.g., PCA, autoencoders)\n\nNote that autoencoders are technically dimensionality reduction, but can help with clustering by reducing your feature set into two dimensional space",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How PCA and DBSCAN work",
    "back": "PCA: PCA simplifies data by reducing dimensions while preserving variance.\n- Standardize the dataset (mean=0, variance=1).\n- Compute the covariance matrix of the data.\n- Perform eigen decomposition of the covariance matrix to find eigenvalues (variances) and eigenvectors (principal components).\n- Select the top kkk eigenvectors corresponding to the largest eigenvalues to reduce dimensions.\n- Transform the data by projecting it onto the new subspace formed by these kkk eigenvectors.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Random Forest vs. Gradient Boosting, and why they call it random forest",
    "back": "- Each tree in an RF is independent, while each tree in GB tries to correct the errors in prior trees\n- RF trees are averaged in any order, but GB trees are generated sequentially, with the last tree being the best one\n\n- Called \"RF\". because we use random subsets of both a) observations and b) features",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "We have to __ our data before PCA",
    "back": "Normalize our data, since PCA is a variance maximizing exercise. If we didn't normalize our data, PCA would project as much weight as possible onto the features with the highest variance",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Describe encoder-decoder architectures (Transformers)",
    "back": "- We use this encoder to transform the input text into a vector in an embedding space, then use a decoder to transform that vector back into some other form\n- Having both an encoder and decoder allows us to transform text in various ways (e.g., from English to German)\n- Transformers use both an encoder and a decoder",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Difference between logistic regression and linear regression (3)",
    "back": "- logistic regression models probability, while linear regression just models the actual response variable\n- we use maximum likelihood or gradient descent, not least squares, to predict the output (though the latter is possible)\n- all probabilities lie between 0 and 1, and the predicted curve wave is an S-shaped curve, not a straight line",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How maximum likelihood works",
    "back": "We try to find B0, B1, etc. such that plugging these estimates into the model for P(x) yields a number close to one for all individuals who had the response variable, and a number close to zero for those who didn't.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "5 steps describing how transformers work",
    "back": "1) We start with some input text, e.g., \"je suis etudiant\". \n\n2) The text is tokenized and converted into embeddings:\n- Token embeddings: Represent the meaning of each token\n- Positional encodings: Capture the position of each token in the sequence\n- Segment embeddings: Distinguish between different sentences or segments (for tasks involving multiple sentences)\n\n3) We pass that text to a stacked set of encoders (the original paper used six encoders), each made up of three components: self-attention layer, a feed-forward neural network layer, and two layer-normalization layers after both the self-attention layer and the feed-forward neural network layer. The encoders are all identical, but don't share weights. Each word in the text is turned into a vector in the first encoder, so each encoding layer receives a list of vectors, one for each word. The order of the words is maintained through each layer. \n\n4) The final output of the last encoder is transformed into a set of key/value attention vectors which are then passed to the first decoder layer.\n\n5) The decoder layers process the word outputs one at a time. These decoder layers are different from the encoder layers in the following ways: \n- they include an extra encoder/decoder self-attention layer in each module\n- they only consider the attention of previously-decoded words, not any words in the future (i.e., they are autoregressive, a.k.a. causal language modeling)\n\nThe output of the decoder is a vector of floats\n\n6) The final layer, the Linear layer, is a fully-connected neural network that takes the vector of floats and turns them into a vector of logits.\n\n7) The outputted vector is transformed into probabilities using a softmax function, and the most likely tokens are converted into words by a tokenizer\n\nhttp://jalammar.github.io/illustrated-transformer/",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why do transformers use self-attention?",
    "back": "- the feed-forward neural network components of encoders and decoders don't store information that connect each word with one another\n\n- self-attention solves this problem by looking at other input words for clues that can lead to a better encoding for the word. It returns a \"context\" vector that relates words to one another.\n\n- transformers use multi-headed self-attention, where each input vector into the decoder is passed through multiple \"representation subspaces\", which are then averaged together using a matrix of weights. these heads can be computed in parallel such that we increase the model's ability to learn complex patterns without significantly increasing computational cost\n\n\nExample: \"The animal didn't cross the street because it was too tired\". Self-attention vectors tells us that the word \"it\" is most associated with the word \"animal\", which helps create a similar encoding for the two words.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What distance function should you use to calculate the distance between two embeddings?",
    "back": "Cosine similarity\n\nCosine similarity can be computed slightly faster using just a dot product\nCosine similarity and Euclidean distance will result in the identical rankings",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Term-frequency inverse-density-frequency",
    "back": "TF-IDF: describes how important a word is in a given document\n\nthe ratio of how often a word appears in a document, divided by the ratio the number of times that word appears in a bunch of documents. the idea being \"which words in this doc are super important to this doc, but not often mentioned elsewhere?\"",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How do you avoid getting stuck in local minima when optimizing over parameters?",
    "back": "- \"Stochastic\" in SGD means that the gradient is based on a single training sample, rather than the whole dataset. This causes the path towards the global cost minimum to \"zig-zag\" in a way that reduces the likelihood of being stuck in a local minima\n- Momentum\n- Shuffle the training set before each epoch (or iteration in the \"standard\" variant)\n- Use an adaptive learning rate to \"anneal\" closer to the global minimum (i.e., simulated annealing); the learning rate should decrease over time so that the descent is less jumpy",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Self-supervised learning",
    "back": "where we automatically parse the classification label from the input data (e.g., what encoders do by masking certain words, then trying to predict what the masked word used to be)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Subword tokenization",
    "back": "A way to tokenize language without a) assigning different ids to similar words (e.g., dog and dogs) or b) losing context via character tokenization\n\n\"Let's do tokenization!\" -> [Let's</w>, do</w>, token, ization</w>, !</w>]\n\nBenefits:\n- Out-of-Vocabulary (OOV) Handling: Subwords allow decomposition of rare or unseen words into smaller, known parts.Example: \"unseenword\" \u2192 \"un\", \"seen\", \"word\".\n- Morphological Richness: Useful for languages with complex inflections (e.g., Turkish, Finnish).\n- Memory Efficiency: Reduces vocabulary size compared to word-level models while capturing more semantic information than character-level models.\n\nPopular tokenization methods include Byte-Pair Encoding (BPE)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What happens when we pass in sequences of different lengths to a language model?",
    "back": "The input array has to be rectangular, so the sequences are padded such that all sequences take on the same length. We then have to tell the Transformer to ignore the padding values so that they don't bias the encodings of the non-padded values. This process is accomplished using a masking vector.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Process for using language models",
    "back": "The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text\n\nNote that the model does not directly output text; it outputs a vector that must be translated using a tokenizer. The component that converts the output to text is called an adaption head (or simply 'head')",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "self-supervised learning",
    "back": "Self-supervised learning obtains supervisory signals from the data itself, often leveraging the underlying structure in the data. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input. For example, as is common in NLP, we can hide part of a sentence and predict the hidden words from the remaining words. We can also predict past or future frames in a video (hidden data) from current ones (observed data). Since self-supervised learning uses the structure of the data itself, it can make use of a variety of supervisory signals across co-occurring modalities (e.g., video and audio) and across large data sets \u2014 all without relying on labels.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to reduce overfitting",
    "back": "- Regularization (L1 vs. L2)\n- Decrease model size / complexity\n- Early stoppage\n- Use a smaller, better set of features\n- Weight decay and/or drop out (for NNs)\n\n- Data Augmentation\n- Transfer learning",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to reduce underfitting",
    "back": "More training data (resampling, data generation)\nIncrease model size / complexity\nTest new features",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Can you describe the end-to-end process of building an ML pipeline?",
    "back": "Aligning on the business problem with stakeholders\nCreating features alongside domain experts\nThe trade-offs associated with different models and metrics\nCross-validation for parameter tuning\nUsing a validation set for model selection\nUsing holdouts to evaluate performance after the model moves into production",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Are you familiar with gradient descent? When is this used? What are a few different ways to perform gradient descent? What does \"stochastic\" mean in SGD?",
    "back": "- The trade-offs of batch vs. gradient descent\nSGD involving calculating the gradient based on a subset of randomly-selected samples\n- Mentioning that SGD being generally preferred for performance optimization reasons\n- SGD also helps to avoid becoming stuck in a local minima by randomizing the inputs of each iteration within an epoch; the added noise introduced by this randomization process helps reduce the likelihood of becoming stuck in a local minima",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What does the term \"learning rate\" mean in deep learning? What are the pros / cons of a low learning rate? How should you set the learning rate?",
    "back": "Low learning rate -> takes longer to train our model; leads to overfitting as we're more likely to get stuck in a saddle point / local minima\nHigh learning rate -> optimizer overshoots the minimum loss and doesn't converge\nFinding the right learning rate -> start with a small learning rate and gradually double it while evaluating over various minibatches. Continue until the loss gets worse, not better, then select a rate below this last point\nTo validate -> measure in-sample and out-of-sample error rates over different folds of data using learning rates that are close to your selection\nSimulated annealing",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why adjust the learning rate for earlier layers vs. later layers?",
    "back": "The intuition is that in the layers closer to the input layer are more likely to have learned more general features -- such as lines and edges, which we won't want to change much. Thus, we set their learning rate low. On the other hand, in case of later layers of the model -- which learn the detailed features, we increase the learning rate -- to let the new layers learn fast.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Difference between loss function and activation function",
    "back": "- Purpose \n - Loss Function: Measures how well the model's predictions match the ground truth. \n - Activation Function: Transforms the output of a neuron into a desired range or format. \n \n- Location in Model \n - Loss Function: Applied at the end of the model (output layer). \n - Activation Function: Applied to the hidden layers and/or the output layer. \n\n- Input \n - Loss Function: Takes the model's predictions and ground truth labels as input. \n - Activation Function: Takes the weighted sum of inputs to a neuron. \n\n- Output \n - Loss Function: A scalar value representing the error (loss) for optimization. \n - Activation Function: A transformed value, often between a range like [0, 1] or [-1, 1]. \n\n- Example Usage \n - Loss Function: Guides model training by minimizing the error using optimization algorithms (e.g., gradient descent). \n - Activation Function: Introduces non-linearity to the model, enabling it to learn complex patterns. \n\n- Examples \n - Loss Function: Mean Squared Error (MSE), Cross-Entropy Loss, Hinge Loss. \n - Activation Function: Sigmoid, ReLU, Tanh, Softmax. \n\n- Dependency \n - Loss Function: Relies on the entire output of the model to compute the loss. \n - Activation Function: Applied to individual neurons in the model during forward propagation.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What is a loss function? What should you consider when picking a loss function? What loss functions have you used in the past?",
    "back": "- Function used to assess goodness of fit relative to a target variable, taking the output of your model\n- Typically used in optimization by finding the min of the loss function. Sometimes synonymous with objective function, though the latter is more general\n- Loss functions have to be differentiable to use with gradient descent, which is why metrics like recall and precision aren't used as loss functions\n\nConsider:\n- Effect on outliers\n- Interpretability\n\nRegression Tasks:\n- Mean Squared Error (MSE): Used when predicting continuous values.\n- Mean Absolute Error (MAE) / L1 Loss: Also used for regression, but less sensitive to outliers than MSE.\n- MAPE\n- Huber Loss: A combination of MSE and MAE, providing a balance between the two.\n\nClassification Tasks:\n- Binary Cross-Entropy Loss: Used for binary classification.\n- Categorical Cross-Entropy Loss: Used for multi-class classification when the classes are mutually exclusive.\n- Sparse Categorical Cross-Entropy Loss: Similar to categorical cross-entropy but used when the classes are integers.\n- Hinge Loss: Commonly used for Support Vector Machines (SVM) and also for certain types of neural networks.\n\nObject Detection Tasks:\n- Bounding Box Regression Loss: A combination of localization and size errors for object detection tasks.\nIntersection over Union (IoU) Loss: Measures the overlap between predicted and ground truth bounding boxes.\n- Focal Loss: Introduced to address class imbalance issues in object detection.\n\nSemantic Segmentation Tasks:\n- Dice Loss: Measures the similarity between predicted and ground truth segmentation masks.\n- Cross-Entropy Loss for Segmentation: Similar to categorical cross-entropy, but applied to each pixel in the segmentation mask.\n- Jaccard/IoU Loss: Measures the overlap between predicted and ground truth segmentation masks.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What's the difference between Adam and vanilla SGD?",
    "back": "The name \"Adam\" is derived from \"adaptive moment estimation,\" highlighting its ability to adaptively adjust the learning rate for each network weight individually. Unlike SGD, which maintains a single learning rate throughout training, Adam optimizer dynamically computes individual learning rates based on the past gradients and their second moments.\n\nBy incorporating both the first moment (mean) and second moment (uncentered variance) of the gradients, Adam optimizer achieves an adaptive learning rate that can efficiently navigate the optimization landscape during training. This adaptivity helps in faster convergence and improved performance of the neural network.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Triplet loss",
    "back": "- Triplet loss is used during model training to learn embeddings where similar items (e.g., faces, images, or text) are close together and dissimilar items are farther apart in the feature space. \n- The user passes in an anchor object (e.g., image of an old rich guy), a similar object to the anchor (e.g., image of an old guy in a suit), and a dissimilar object to the anchor (e.g., a young teenager). In this way, the model learns which labels are similar to one another.\n- Differs from silhouette coefficient because the former is used during model training as a loss function, whereas the latter is used to evaluate the results of a cluster. Can't use silhouette coefficient for model training as it's not differentiable",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Intersection Over Union (IOU)",
    "back": "An evaluation concept for object detection and segmentation where we identify the percent of shared pixels between two polygons / bounding boxes.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Object Detection Evaluation Metrics",
    "back": "- Average Precision: The weighted mean of precisions achieved at several different recall thresholds for a single Intersection over Union (IOU), grouped by class.\n- AP Averaged over IOUs: The average of several AP metrics across IOU thresholds, grouped by class labels.\n- Mean Average Precision: The average of several AP metrics across class labels, grouped by IOU thresholds.\n- Mean Average Precision Averaged over IOUs: The average of several mAP metrics across class labels.\n- Average Recall: The average of several recall metrics across IOU thresholds, grouped by class labels.\n- Mean Average Recall: The average of several AR metrics across class labels.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Tricky parts of computer vision tasks",
    "back": "- Computational requirements involved with comparing large matrices representing different images\n- The different dimensions that images can have (e.g., Synthetic-aperture radar vs. RGB images)\n- Dealing with different geometries (e.g., bounding boxes vs. polygons vs. segmentations) in an efficient way",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Is overfitting or underfitting a bigger issue with small datasets?",
    "back": "Overfitting\n\nHigh Variance in Small Datasets: Limited datasets are often not representative of the entire population, leading to high variance. A model trained on such data might capture noise or outliers specific to the training set, causing overfitting.\nLack of Generalization: With few examples, the model cannot learn the broader trends and relationships within the data, making it prone to learning specific details that do not generalize well to unseen data.\nValidation and Testing Difficulties: Small datasets make it harder to have separate training, validation, and test sets. This leads to less reliable estimates of model performance and a higher chance of tuning the model too closely to the training data, resulting in overfitting.\nComplex Models with Few Data Points: If a complex model is used with a small dataset, the model has enough capacity to learn the training data very well, but it will likely fail to generalize to new data due to overfitting.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why is cosine similarity used as the preferred distance measure for RAG?",
    "back": "cosine similarity is effective for capturing the angle between high-dimensional vectors, which indicates the similarity in their direction rather than their magnitude. By using cosine similarity, you can identify how closely related two embeddings are in the semantic space of the model.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "BLEU vs. ROUGE",
    "back": "BLEU and ROUGE are both metrics used to evaluate the quality of machine-generated text, but they differ in their focus, calculation method, and use cases:\n\nBLEU\nFocuses on precision, or how much of the machine-translated text matches the human reference text. BLEU is primarily used for machine translation tasks.\n\nROUGE\nFocuses on recall, or how much of the human reference text is contained in the machine-generated text. ROUGE is primarily used for text summarization tasks.\nBoth metrics use n-gram overlap to measure similarity between the machine-generated output and the reference text. They range from 0 to 1, with 1 meaning the sentences are exactly the same.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Layer normalization",
    "back": "- applied after each sub layer (e.g., within each encoder, there is one layer normalization after the self-attention step and another after the FF NN step)\n\n- it helps mitigate the internal covariate shift problem by normalizing inputs (e.g., trimming outliers), making training more stable.\n\n- Normalized inputs tend to result in faster model convergence.\n\n- makes the model less sensitive to parameter initialization.\n\n- Layer normalization can help the model generalize better to unseen data.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pre-training objectives for transformers: why they're necessary and three examples",
    "back": "- the pre-training objectives are examples of self-supervised learning, and are how models are able to learn relationships in language\n\n- Masked Language Modeling (MLM): predicts randomly masked words in the input, forcing the model to understand context from both the left and right. Used by BERT and DeBERTA.\n- Next Sentence Prediction (NSP): predict whether two sentences follow each other in the text. helps in learning relationships between sentences. Used by BERT.\n- Autoregressive language modeling: Predicts the next token based only on the preceding context. Used by GPT-3\n- Text Infilling: Replacing spans of words with \"[MASK]\", and then predicting what words are missing.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "BERT vs. DeBERTa vs. GPT-3",
    "back": "# Architecture:\n- BERT: Uses a bidirectional encoder architecture; 340M parameters; 512 token limit\n - DeBERTa: Uses a bidirectional encoder architecture; 1.4B parameters; longer token limit\n- GPT-3: unidirectional decoder, 96 attencion blocks, 175B parameters; 2048 token limit\n\n# Training Objectives\n- BERT and DeBERTa use masked language modeling (MLM) as the primary pre-training task\n- BERT also uses next sentence prediction (NSP)\n- GPT-3 uses autoregressive language modeling, where we predict the next token based on proceeding tokens\n\n# Strengths:\n- BERT and DeBERTA: Language understanding or tasks that require context, like Q&A and fill-in-the-blank tasks\n- GPT-3: Text generation",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What is the point of applying a SoftMax function to the logits output by a sequence classification model?",
    "back": "- It bounds the output between 0 and 1\n- The total sum of the output is then 1, resulting in a possible probabilistic interpretation",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Give examples of regularization parameters for LightGBM, BERT, Logistic regression, and Siamese Nets",
    "back": "Logistic Regression\n- L1 regularization strength (lasso)\n- L2 regularization strength (ridge)\n- Elastic net mixing parameter\n- Early stopping tolerance (halt training when the performance improvement on some validation set fails to exceed some delta value)\n- Maximum number of iterations (avoids overtraining)\n\nLightGBM / XGBoost\n- Learning rate (simulated annealing)\n- Number of estimators (the number of individual decision trees)\n- Maximum tree depth (how many breakpoints each estimator can consider)\n- Minimum child weight (only create new tree nodes if they pass some threshold)\n- Subsample ratio (the fraction of training to be randomly sampled for each estimator)\n\nBERT\n- Dropout rate (probability of randomly setting a given neuron's weight to zero)\n- Weight decay (penalizes large weights to more evenly spread the decision-making criteria across the network)\n- Attention dropout rate (randomly dropping weights from the attention vector in training)\n- Hidden dropout rate\n- Layer-wise learning rate decay12\n\nSiamese Nets for Computer Vision\n- Contrastive loss margin\n- Triplet loss margin\n- Weight decay\n- Dropout rate\n- Data augmentation strength",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How can you get the confidence of a transformer's output?",
    "back": "- Max Softmax: Use the highest softmax probability as the confidence score. This assumes higher probability indicates greater certainty2.\n- Softmax Difference: Calculate the difference between the highest and second-highest softmax probabilities. A larger gap suggests higher confidence2.\n- Softmax Variance: Compute the variance of all softmax probabilities. Lower variance may indicate higher confidence2.\n- Softmax Entropy: Calculate the entropy of the softmax probabilities. Lower entropy suggests higher confidence2",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four techniques for model compression",
    "back": "Quantization\n- Using 8-bits to represent model parameters instead of 32 bits\n- Reduces memory requirements and improves model speed at the expense of accuracy\n\nPruning\n- Removing parameters, tree edges, or neural network edges\n\nKnowledge distillation\n- Training a smaller model to match the results of a larger model\n\nFactorization\n- Factorization Machines (FM) are supervised learning models designed to handle sparse, high-dimensional data. They extend the concept of matrix factorization to capture feature interactions in addition to latent factors.",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of ranking models:",
    "back": "- Collaborative filtering: filters two matrices (one that maps user similarities to content, and another that maps content similarities to other content) to determine other content the user might like\n- Learning to rank (LTR) models (i.e., LambdaMART is a tree-based model; RankedBoost is a NN): we train a model to predict relevance scores using a variety of features (e.g., user interaction, prior shows watched), which are then sorted to output rankings",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Types of re-ranking and Learning to Rank models (3)",
    "back": "- pointwise: the model outputs relevance scores which can then be ranked\n- pairwise: comparing two outputs in a binary classification problem to determine which should be ranked higher\n- listwise: rank orders the full list, which can improve results by considering the context between each individually-ranked element",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Calculation for precision @ k",
    "back": "(# of relevant items in top k predictions) / k",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Calculation for recall @ k",
    "back": "(# of relevant items in top k predictions) / (total # of relevant items)",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Innovations required to use LLM for embeddings",
    "back": "- Instead of masking future text in the decoder-only architecture, we predict each word using bi-directional context\n- Combining the output vectors from all of the tokens together using mean pooling (take the average for each position in the vector) or max pooling (take the mean for each position in the vector)\n\nhttps://huggingface.co/spaces/mteb/leaderboard",
    "tags": [
      "machine_learning"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "8 Architectural components of a web app",
    "back": "\u25cb Client\n \u25cb DNS (usually owned by a third party)\n \u25cb CDN (usually owned by a third party)\n \u25cb API Gateway (handles authentication)\n \u25cb Queue (Kafka for log to disc, RabbitMQ for in-memory)\n \u25cb Load balancer + reverse proxy (forwards requests to various servers, and caches the result for other clients)\n \u25cb Web server (does computations)\n \u25cb Cache (retrieves recently-queried data quickly)\n \u25cb Database (persists data)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "CDN vs. cache",
    "back": "\u25cb CDN serves static content\n \u25cb CDN is usually operated by a third-party",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "2 benefits of horizontal scaling",
    "back": "\u25cb No limit to CPU / memory / GPU usage you're adding to your overall system\n \u25cb Provides redundancy and failover protection",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What is master/slave model and 3 benefits of database replication",
    "back": "\u25cb Writes happen only on the master, while slaves are read-only\n \u25cb Benefits\n \u00a7 Improved read performance\n \u00a7 Improved reliability; slaves can get promoted to master if master goes down\n \u00a7 Availability: data is replicated across different locations",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "When to use cache and three benefits",
    "back": "\u25cb Use when data is read frequently but modified infrequently\n \u25cb Benefits\n \u00a7 Improved performance\n \u00a7 Ability to scale cache independently\n \u00a7 Reduces database workloads",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "2 common problems with CDNs and caches",
    "back": "\u25cb Have to strategically expire or invalidate data to prevent inconsistencies\n \u25cb Have to set an eviction policy",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Differences in stateful vs. stateless architectures",
    "back": "\u25cb Stateful: state is maintained on a particular web server using cookies. Issue is that the same user will have to be connected to the same web server each time, which causes overhead for the load balancer\n \u25cb Stateless: state is maintained on a separate database that connects cookies / telemetry with program params. Typically stored ina NoSQL database",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Differences between document stores (e.g., DynamoDB) vs object stores (S3)",
    "back": "\u25cb Both are NoSQL databases\n \u25cb NoSQL stores allow indexing of actual content within objects, while object stores don't\n \u25cb Document stores have stronger object size limits (400KB), but very high access speeds\n \u25cb Object stores are great at storing large files, but have lower access speeds",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 issues with sharding",
    "back": "\u25cb Data may have to be resharded if one shard grows too large, or if the data becomes unevenly distributed\n \u25cb If one shard is more popular than others, it may cause performance issues (celebrity problem)\n \u25cb It's expensive to perform join operations across database shards",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "8 scaling strategies",
    "back": "- Monitoring (MENTION THIS!) for bottlenecks and points of failure\n- Aggregate: Aggregate your data in such a way that you have to store/process less of it\n- Compress: Simple compression is fast; compress data before sending it over the internet\n- Cache: Uses caches as much as you can (caches, CDNs, and DNS) to reduce disk reads / seeks\n- Decouple: Split tiers into individual services; make the system as async as possible. Keep your web tier stateless so that your load balancer can work more effectively. \n- Distribute: Scale horizontally. Shard your data. Use consistent hashing to distribute the load amongst new / deleted servers. \n- Localize\n- Replication: Build redundancy into your web servers, databases, caches, and workers to avoid data failures. Use master/worker distinctions to manage who can actually read/write. Support multiple data centers to get data closer to users.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "99.99% availability is equivalent to __ seconds of downtime per day",
    "back": "\u25cb 8.64 seconds",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What are WebSockets?",
    "back": "- Yes, using WebSockets\n \u25cb HTTP is client-initiated, so it's not trivial to send messages from the server\n \u25cb Polling: the client periodically asks the server if there are messages available\n \u25cb Long polling: client holds the connection open until there are actually new messages available, or a timeout threshold is reached\n \u25cb WebSocket: a bi-direction and persistent connection formed when the client and server handshake on the TCP layer (one level below HTTP); the connection stays open on port 80 or 443 and is maintained via heartbeats. Unlike with HTTP (which happens at the Application Layer), packets aren't inspected by the API before being passed to the server",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Formula for calculating QPS and peak QPS",
    "back": "\u25cb QPS = [DAU] * [average daily queries per second]\n \u25cb Peak QPS = 2 * QPS",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "5 step process for solving system design interview",
    "back": "*Requirements*: Ask clarifying questions about functional requirements (i.e., problem we're trying to solve) and non-functional requirements (U/S/P/A vs. C /C)\n\n*Write it down*: Write down the problem that we're trying to solve, as well as a list of functional (what it needs to do) and non-functional (properties of the system related to scalability, performance, and availability) requirements on the whiteboard. Functional requirements can be written in terms of functions (e.g., writeCameraFeed)\n\n*High-Level Design*: Start with a high-level system, usually putting the database down first. Draw arrows to indicate how data moves in and out. \n\n*Data Details*: Add new components to the map, describing how data is stored, transferred, and processed. Write down the database schema to identify the granularity of data that needs to be stored. Name-drop technologies\n\n*Recap*: Share a recap with bottlenecks and trade-offs",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Common rate limiter algos",
    "back": "\u25cb Token bucket algorithm: Tokens are put into a container with a pre-defined capacity for each user, and popped off whenever there is a request. If there aren't enough tokens when a token comes through, the request is dropped (returns HTTP 429)\n \u25cb Leaking bucket algo: Doesn't use tokens; requests themselves are added to the bucket, and dropped if the bucket is full",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Two issues with working in a distributed environment",
    "back": "\u25cb Race conditions: If two workers are incrementing a count at the same time, how do you ensure the count is right? (answer: sorted sets data structure in Redis, or Lua scripts)\n \u25cb Synchronization: how do you ensure the same client is using the same web server / API over time (answer: stateless architecture to store the data in a cache layer)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "CAP Theorem",
    "back": "\u25cb Consistency: you always see the correct data\n \u25cb Availability: your requests are always handled by a server (example: people should both be able to edit at GDoc even if one is offline, and the changes should get merged later)\n \u25cb Partitioning tolerance: your data is replicated across nodes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four bottlenecks in a distributed system",
    "back": "\u25cb Compute time\n\u25cb Bandwidth\n_____ CPU bandwidth\n_____Memory bandwidth\n_____Network bandwidth\n_____Disk bandwidth\n \u25cb Scheduler overhead\n \u25cb Serialization costs",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Common textual vs. binary serialization formats",
    "back": "Textual: JSON, CSV\n\nBinary: \n \u25cb Protocol buffers (protobufs) and Avro: stores data in rows, so good for transactions as it can be split row-wise\n \u25cb Parquets (.par): column-oriented, so better for wide data as it can be split column-wise",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Reverse proxy",
    "back": "answers calls to multiple Ips, providing a unified interface to the public",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "ACID",
    "back": "used for handling transactions\n \u25cb Atomicity -> put all writes in a write-ahead log before actually writing them to disc, then mark that as committed as you actually had them to the database. in this way, all commits have to either succeed or fail\n \u25cb Consistency -> All transaction will be held to the same pre-defined rules\n \u25cb Isolation -> transactions won't affect each other\n \u25cb Durability -> once committed, it will remain in the system",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of storage",
    "back": "Block storage: data is broken into blocks of equal sizes, and given a unique ID that can be used to query the block\n\nFile storage: Data is stored in files and directories, which can become cumbersome over time\n\nObject storage: Story entire, unstructured blobs of memory at once. Slower than block or file storage, but less limitations on the amount of space that the blobs can take up. Great for cold storage.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Message queue",
    "back": "A message queue is a queue that routes messages from a source to a destination, or from the sender to the receiver. It follows the FIFO (first in first out) policy. The message that is sent first is delivered first. Message queues facilitate asynchronous behavior, which allows modules to communicate with each other in the background without hindering primary tasks. They also facilitate cross-module communication and provide temporary storage for messages until they are processed and consumed by the consumer.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of distributed event storage systems, and what these systems are used for",
    "back": "Kafka, RabbitMQ\n\nKafka is a distributed system consisting of servers and clients that communicate through a TCP network protocol. The system allows us to read, write, store, and process events. Kafka is primarily used for building data pipelines and implementing streaming solutions.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "xx",
    "back": "",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Six types of requirements to ask during system design interview",
    "back": "U: Users\n- Job to be done? \n- Who will use the system?\n- How many users?\n\nM: Metrics\n- How do we measure success?\n\nP: Performance\n- How many read/write queries per second? (split by read/write or estimate the read:write ratio)?\n- How much data is read/written per request?\n- Can their be spikes in traffic?\n- Convert into bandwidth / storage / network / memory (e.g., cache) estimates\n\nA: Availability vs. Consistency and Reliability\n- What uptime do we need to maintain?\n\nC: Cost\n- Should we use open source frameworks to minimize costs?\n- Should we minimize the cost of maintenance?\n\nP: Privacy & Legal\n- Should we be concerned about privacy and legal (e.g., The Great Firewall with China)?\n- Can we trust cloud providers with our data, or do we need to host on-prem?",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "3 downsides of storing aggregated data instead of raw events",
    "back": "- Can only query data the way it was aggregated\n- Impossible to fix errors in the aggregation logic without a copy of the raw events\n- Requires aggregation pipeline",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Read quorum (used on Cassandra)",
    "back": "When multiple nodes containing replica data have to agree on the output as a means of avoiding returning stale data",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Four types of NoSQL databases",
    "back": "- Column (Cassandra)\n- Document (MongoDB)\n- Key-value\n- Graph (Neo4J)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Advantages / disads of Cassandra",
    "back": "- Easy to set up distributed replication: Read and write throughput increases linearly (client requests are sent to any node, which then forwards that request to the right node)\n- Fault tolerant\n- Works well with time-series data\n- Supports async, masterless replication\n\n- not ACID; high availability and write performance makes it poor for transactional systems which require immediate consistency",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ads / Disads of MongoDB",
    "back": "- Handles indexing for you, making it useful as a document database\n- Very flexible\n\n- doesn't support transactions (slow writes)\n- schema is too flexible; can't assume the structure of stored data",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to achieve:\n\nScalability (3) -> \n\nHigh Throughput (4) -> \n\nNot lose data when it crashes (2) -> \n\nHandle instances where the DB is unavailable (1) ->",
    "back": "Distribute, Chunk, and Caches / phased storage (e.g., cold storage, which uses an object store)\n\nCache, Compress, Aggregate, Index\n\nReplicate, Distribute\n\nReplication / checkpointing + distributed queues\n\nDead-letter queue, a place to store events that can't be written",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to avoid duplicates being written when using partitioned queues",
    "back": "Use a distributed cache that stores event_ids for the last 10 minutes, and only does the write if the event isn't in the set",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How do blocking systems work",
    "back": "When a client sends a request to a service using sockets, a new thread is spun-up on the service to handle the request. Both the socket making the connection and the thread itself are blocked for other uses during response\n\nBlocking systems create one thread per connection\n\nAlternative is to use a single thread to handle multiple concurrent requests via a queue. This is higher throughput at the expense of complexity",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three ways to handle retries",
    "back": "- Exponential backoff: Retries take longer to kick-off each time\n\n- Jitter: retries don't start immediately after failure\n\n- Circuit breaker: Track the number of failures in a downstream service, and stop sending requests for a while",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "TCP vs HTTP load balancers",
    "back": "TCP (used by WebSocket) load balancers simply forward packets without inspecting them, making them adapt at handling millions of requests per second. The connection doesn't close after each packet is sent.\n\nHTTP load balancers look inside the message (e.g., a cookie) before deciding what to do with the message, then close the connection after sending it.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How is Zookeeper used?",
    "back": "Zookeeper is used to manage clusters of nodes by keeping track of the host names within each cluster, along with each hosts' health / availability over time\n\nZookeeper can also elect leaders and followers to more easily manage leader-follow database relationships\n\nZookeeper allows concurrent reads but only linear writes, which means it isn't ideal when you have lots and lots of writes going on at a point in time.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to avoid load balancers being a bottleneck (2)",
    "back": "- Have a secondary load balancer that just monitors the health of the primary, and takes over if it goes down\n- Distribute across several load balancers",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of replication",
    "back": "Single leader - one node does the writing, the others just do reading. The leader checks-in on the followers to make sure they're alive, and only sends a success response back when the content has been replicated to a certain number of nodes\n\nMulti-leader - similar to single-leader, but we have multiple leaders that handle different partitions\n\nLeaderless (like how Cassandra works, where every node has separate partitions)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "API Gateway vs. Load Balancer",
    "back": "API Gateways come before the load balancer and handle authentication and identity management. This layer can be scaled / distributed for higher throughput.\n\nLoad balancers take authenticated requests and distribute them across partitioning services.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Commonly used tech for the following components:\n\nClient-side -> \n\nLoad Balancing -> \n\nMessaging Systems -> \n\nData Processing -> \n\nStorage -> \n\nCache -> \n\nMessage Brokers -> \n\nMonitoring Services -> \n\nLoad testing ->",
    "back": "Client-side -> React, Flutter\n\nLoad Balancing -> NGINX, AWS ELB (Elastic Load Balancer)\n\nMessaging Systems -> Apache Kafka, RabbitMQ\n\nData Processing -> Spark, Flink\n\nNoSQL Storage -> Cassandra, HBase for columnar storage (good for time-series); Hadoop or AWS Redshift for raw events; S3 for objects, MongoDB\n\nSQL Storage -> PostgreSQL, MySQL, SQLLite\n\nCache -> Redis\n\nMessage Brokers -> RabbitMQ, Kafka\n\nMonitoring Services -> AWS Cloudwatch, Prometheus, Datadog\n\nLoad testing -> Apache JMeter",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of performance testing",
    "back": "Load testing -> test system under various loads to see if we get the results we expect back\n\nStress -> heavy load to identify bottlenecks and identify breaking point\n\nSoak -> typical load for a long period of time to identify memory leaks",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Roles of a front-end web service (9)",
    "back": "- SSL termination (decrypts request, done using a TLS HTTP proxy)\n- Authentication\n- Request validation (required parameters are present. Data values within acceptable ranges)\n- Server-side encryption (encrypts message before it's stored in databases) \n- Caching\n- Rate limiting\n- Request dispatching\n- Request deduplication\n- Usage data collection",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to achieve durability",
    "back": "Replication!",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pull vs. push model",
    "back": "Pull: consumers bombard servers with requests until they get back the data they need\n\nPush: consumer is notified when messages arrive in the queue",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "All databases should have:",
    "back": "their own write service",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "To improve availability:\n\nTo improve consistency:",
    "back": "- Replicate (causes consistency issues)\n- Partition\n\n- Enforce quorums",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How bloom filters work (4)",
    "back": "- An array of length n is initialized with only zeroes\n- New items are hashed in multiple ways (e.g., 3 different hashes), then we take the module of those hashes by n\n- Those indices in the array are flipped to one\n- When we want to check if an item may be in the set, we run that item through all the hash functions and check those indices. if they are all equal to one, than the item *may* be in the set. if not, then it's definitely not\n\n- Bloom filters trade accuracy for lookup speed in relation to a normal hash table lookup",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Topics to mention during your high-level design (3)",
    "back": "Bottleneck: It's important to identify the most likely bottleneck, and justify your decision with a back-of-the-envelope calculation\n\nSingle Point of Failure: The one component that could cause the whole system to crash\n\nFailure Scenarios: Other scenarios that would be disasterous for your system",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of functional requirements to write down",
    "back": "\"Users should be able to...\"\n- Search?\n- Notifications?\n- Popular?\n- Share?\n- Privacy?\n- Sort order of content?",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of non-functional requirements to write down",
    "back": "\"The system should...\"\n- Logging\n- Alerts \n- Pagerduty\n- Deployment\n- Failure scenarios",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What to say in a good recap (6)",
    "back": "- Summarize problem and solution\n- Deployment plan\n- Bottlenecks\n- Potential improvements / scaling strategies\n- Error cases (server failures, network loss, etc.)\n- Monitoring and measurement",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to speed-up database operations",
    "back": "- Sequential reads / writes are faster than random reads / writes\n- Keep data that is frequently read or written together",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "If we just want to optimize for writes...",
    "back": "Don't bother using an index, just use a write-ahead log to write all data sequentially\n\nTo read from the bottom of a write-ahead log, we have to scan from bottom to top to find a variable",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Types of database indexes and pros / cons",
    "back": "Hash index: hash the key and store it in memory\n- O(1) reads and writes\n- Requires storing all the keys in memory\n- Actual data isn't stored in any particular order\n\nB+ Tree: organize keys in a tree structure on disk\n- Similar data is stored together on disc, which means fasters reads of sequential data\n- O(logn) reads / writes (slower than a hash index)\n\nLSM Trees and SSTables (used by Cassandra): writes go to an in-memory binary search tree until that tree grows too large, then they are flushed into sorted tables on disc\n- Writes and reads in O(n)\n- Fast writes to memory\n- Slow reads since we have to check in-memory and, sometimes, on disc",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to ensure that database writes are serializable (e.g., able to be processed without race conditions)",
    "back": "- run on a single thread\n- pessimistic concurrency control: use locks to stop conflicting transactions\n- optimistic concurrency control: allow transactions to proceed with no locks, but abort transactions that see that the data has been modified (better if there aren't many overwrites)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Do all databases support ACID transactions?",
    "back": "No, since they slow down read/write operations in exchange for consistency (by being able to handle concurrency)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Column vs. row-store, and when to use each",
    "back": "- Column-store: store all values for a given column together. better if you're only ever interested in a few columns at a time for a given row\n\n- Row-store: store all values for a given row together. usually faster for writes where we're consistently writing new records as rows",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Synchronous vs. async replication",
    "back": "Synchronous (Strong Consistency): All replicas must acknowledge the new value for the commit to be considered complete. If a single replica is down, then we cannot commit the write.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Two-phase commit",
    "back": "- A scheduler tells two consumers to prepare to commit data, then it tells them to actually complete the commit\n- Slow since the scheduler has to wait for the slowest node to respond\n- High consistency\n- Nodes can't do other work while waiting for the final commit confirmation",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Server Sent Events vs. Websockets",
    "back": "- SSE can automatically re-establish a connection that failes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Kafka vs. RabbitMQ",
    "back": "RabbitMQ -> In-memory for maximum through-put\n- Very fast since queue is held in memory\n- Can't replay previous tasks since they are deleted after processed (note that there are options now to preserve RabbitMQ tasks)\n- Tasks aren't guaranteed to be processed in order\n\nKafka, Amazon Kinesis -> Log based message broker\n- Writes to disk sequentially, which is faster than random writes\n- Message are all read in correct order; no round robin. if you want to process in parallel, then you have to spin up a second broker to handle a particular set of keys\n- slow messages create a bottleneck\n- messages are durable and can be replayed if the system does down\n- Used for change data capture since we really care about preserving the order of changes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Names for three actors in a stream processing system",
    "back": "- Producers: produce data\n- Consumers: do stuff with data asyncronously\n- Broker: Holds the data from producers until it's ready to be consumed by a consumer",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Example of a database that uses multi-leader replication with LSM + SST indexes",
    "back": "Cassandra, Snowflake",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Example of a database that uses single leader replication",
    "back": "MySQL, Postgres, Redis",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three ways to load your cache",
    "back": "- Write-back: Data written to cache, later to storage. Fast writes, risk of data loss.\n- Write-through: Data written to cache and storage simultaneously. Consistent, slower writes.\n- Write-around: Data written directly to storage, then added to cache after a user queries the cache and encounters a cache miss.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Use case for Apache Flink",
    "back": "- processes events from a Kafka log, so that you can increment some database value every x events",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ways to handle the case where your database and/or message broker goes down before an acknowledgement is received (which would double-count a log event)",
    "back": "- Two-phase commit (slow)\n- Idempotency key, where the database stores the last event ID that it processed (downside is that you'd have to store the key several times unless you partition it so that a particular Spark Streaming instance handles all publishes to a given row)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Zookeeper",
    "back": "Key value store that helps you map specific keys to their partitions so that you know which servers / caches to hit for specific requests\n\nNote that Zookeeper is no longer used in Kafka after being phased-out for less heavy internal dependencies",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Examples of monitoring tools to monitor PRE metrics",
    "back": "New Relic, Datadog APM",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Five strategies to improve API performance",
    "back": "- Pagination: returning some but not all results at once\n- Async logging: log to an in-memory buffer which is occasionally flushed to disk, rather than logging directly to disk\n- Caching\n- Payload compression (e.g., zipping a json payload before sending) to reduce need for network bandwidth\n- Connection pooling => avoiding the need to open and close db connections by maintaining a pool of open connections that are used at execution time",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "HTTP status code meanings (4)",
    "back": "200+: success\n300+: redirect\n400+ (e.g., 404): client error\n500+: server error",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What does an API gateway do? (7)",
    "back": "- Validates the HTTP request\n- Checks the allow / deny list\n- Talks to an auth provider for authentication and authorization\n- Applies rate limiting logic (rejecting the request if it's over the limit)\n- Finds the relevant service to match the request to by matching\n- Forms the request and sends it to the appropriate service\n- Help with caching, logging, and circuit breaking",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What does a reverse proxy do? (4)",
    "back": "- Load balancing\n- Protecting from a DDOS attack\n- Cache static content\n- Encrypt and decrypt SSL communications",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Idempotency",
    "back": "the main idea is to only complete a particular action once, even if the request is sent to the server multiple times. In practice, we pass a unique key (idempotency key) with each request so that the server can assess whether it's seen the request before",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Inverted Index",
    "back": "maps tokens to their locations in text documents, allowing for efficient search. Used in Lucene.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How are SQL commands executed? (8)",
    "back": "1) A SQL statement is sent to the database via a transport layer protocol (e.g.TCP).\n2) The SQL statement is sent to the command parser, where it goes through syntactic and semantic analysis, and a query tree is generated afterward.\n3) The query tree is sent to the optimizer. The optimizer creates an execution plan.\n4) The execution plan is sent to the executor. The executor retrieves data from the execution.\n5) Access methods provide the data fetching logic required for execution, retrieving data from the storage engine.\n6) Access methods decide whether the SQL statement is read-only. If the query is read-only (SELECT statement), it is passed to the buffer manager for further processing. The buffer manager looks for the data in the cache or data files.\n7) If the statement is an UPDATE or INSERT, it is passed to the transaction manager for further processing.\n8) During a transaction, the data is in lock mode. This is guaranteed by the lock manager. It also ensures the transaction's ACID properties.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "The value of change data capture (CDC)",
    "back": "- updating downstream data (e.g., replicas on other nodes to maintain consistency in a distributed database; data lakes used for analytics, etc.). when a transaction is added to a database, the log of that transaction (the CDC log) can be added to a Kafka queue and processed by other systems\n- CDCs can help with consistency when a node fails: that node can read all the CDC changes as it boots back up\n- CDC can also facilitate integration and ETL processes in downstream systems\n\n- Debezium is commonly used to capture row-level changes from databases and publish these changes as event streams to Kafka topics",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **BigQuery**",
    "back": "- SQL vs. NoSQL vs. Other: Other (Data Warehouse)\n - Replication Strategy: Full Table (Entire dataset replicated across multiple locations)\n - Index Type: Columnar\n - Optimized for: Analytics (Large-scale data processing and complex queries)\n - Availability: High (Distributed architecture ensures continuous operation)\n - Consistency: Strong (Ensures all reads reflect the latest writes)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **Cassandra**",
    "back": "- SQL vs. NoSQL vs. Other: NoSQL\n - Replication Strategy: Multi-Master (All nodes can accept writes)\n - Index Type: LSM-Tree\n - Optimized for: Writes (High-throughput write operations)\n - Availability: High (No single point of failure)\n - Consistency: Eventual (Reads may not reflect the most recent writes immediately)\n - Supports ACID Transactions: No",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **DynamoDB**",
    "back": "- SQL vs. NoSQL vs. Other: NoSQL\n - Replication Strategy: Multi-Region (Data replicated across multiple AWS regions)\n - Index Type: Hash Index\n - Optimized for: Reads/Writes (Fast, scalable performance for both)\n - Availability: High (Automatic multi-region replication)\n - Consistency: Eventual (Consistency across all copies of data is reached within seconds)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **MariaDB**",
    "back": "- SQL vs. NoSQL vs. Other: SQL\n - Replication Strategy: Master-Slave (One primary node, multiple read replicas)\n - Index Type: B-Tree\n - Optimized for: Reads (Efficient query processing and indexing)\n - Availability: High (Failover to replicas if master fails)\n - Consistency: Strong (All reads reflect the latest writes)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **MongoDB**",
    "back": "- SQL vs. NoSQL vs. Other: NoSQL\n - Replication Strategy: Master-Slave (Primary node with secondary replicas)\n - Index Type: B-Tree\n - Optimized for: Reads (Efficient for read-heavy workloads)\n - Availability: High (Automatic failover to secondary nodes)\n - Consistency: Eventual (Reads from secondaries may be slightly outdated)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **MySQL**",
    "back": "- SQL vs. NoSQL vs. Other: SQL\n - Replication Strategy: Master-Slave (One primary node, multiple read replicas)\n - Index Type: B-Tree\n - Optimized for: Reads (Efficient query processing and indexing)\n - Availability: High (Failover to replicas if master fails)\n - Consistency: Strong (All reads reflect the latest writes)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **Postgres**",
    "back": "- SQL vs. NoSQL vs. Other: SQL\n - Replication Strategy: Master-Slave (Primary server with one or more standby servers)\n - Index Type: B-Tree\n - Optimized for: OLTP (Online Transaction Processing)\n - Availability: High (Hot standby ensures quick failover)\n - Consistency: Strong (All reads reflect the latest writes)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **Redis**",
    "back": "- SQL vs. NoSQL vs. Other: NoSQL\n - Replication Strategy: Master-Slave (Single master with one or more replicas)\n - Index Type: Hash Index\n - Optimized for: In-Memory Ops (Ultra-fast data access and processing)\n - Availability: High (Sentinel provides automatic failover)\n - Consistency: Eventual (Replicas may lag behind the master)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **Snowflake**",
    "back": "- SQL vs. NoSQL vs. Other: Other (Data Warehouse)\n - Replication Strategy: Multi-Cluster (Data distributed across multiple compute clusters)\n - Index Type: Columnar\n - Optimized for: Analytics (Large-scale data processing and complex queries)\n - Availability: High (Redundant storage and compute resources)\n - Consistency: Strong (Ensures all reads reflect the latest writes)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "- **SQLite**",
    "back": "- SQL vs. NoSQL vs. Other: SQL\n - Replication Strategy: N/A (Local file-based, no network replication)\n - Index Type: B-Tree\n - Optimized for: Local Storage (Embedded databases for applications)\n - Availability: N/A (Single-user, local database)\n - Consistency: Strong (All operations are immediately visible)\n - Supports ACID Transactions: Yes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Quadtree",
    "back": "- a data structure which recursively divides a geographic area until every region contains n entities\n- Useful for building an index of businesses since you want to be able to find ~100 businesses quickly using an index, but the density of businesses will greatly vary by urban / rural",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "An alternative to sharding is",
    "back": "read replicas, where we just create replicas of the data using CDC to avoid sharding the data (since sharding requires new logic in the application layer)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Caching isn't appropriate when",
    "back": "- you're access data that is fairly small such that it can already be stored in-memory on the database\n- We can just use read replicas to improve throughput, since the database queries against the replicas will be fast",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "What's a lightweight messaging service held in-memory?",
    "back": "- Redis Pub / Sub, a messaging pattern that enables real-time communication between publishers and subscribers through WebSocket connections. Publishers send messages to specific channels, while subscribers receive messages from channels they are interested in, without needing to know about each other\n- Delivery isn't guaranteed\n- Messages are delivered right away\n- No persistence (messages can be dropped); use Kafka if you care about persistence",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Time to Live",
    "back": "- a setting in Redis and other caches where you can define how many seconds you want a key to exist for\n- Keys are expired if they're called at the TTL has passed, or if they're randomly checked and found to have expired",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Downsides of WebSocket",
    "back": "- the servers are stateful, which means a) they must be aligned to \"drain\" (i.e., close all connections) before shutting down or updating the service and b) servers may have to share state \n- Requires memory and processing power to maintain\n- Security considerations since security tools can't analyze each packet\n- No built-in authentication\n- Can't automatically recover lost connections",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ways to improve availability",
    "back": "- Have a standby node that takes over if the leader goes down\n- Implement redundancy at hardware, network, and data levels\n- Use load balancing to distribute traffic across multiple servers\n- Employ failover clustering for automatic switchover to backup systems\n- Implement geographic distribution of resources for disaster recovery\n- Utilize multi-datacenter replication for geographic redundancy",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Ways to improve consistency",
    "back": "- Use consensus algorithms like Paxos or Raft to ensure agreement across nodes6\n- Implement quorum systems to balance consistency needs6\n- Apply event sourcing and CQRS patterns to separate read and write models6\n- Use versioning and conflict resolution strategies for eventual consistency6\n- Employ strong consistency models for critical transactions requiring immediate updates210\n- Implement distributed transactions to maintain data integrity across multiple nodes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Modem alternative to zookeeper",
    "back": "- etcd:\n- ZooKeeper uses strong consistency, while etcd employs linearizability12.\n- etcd is easier to setup and has more client libraries (where Zookeeper is more limited and written in Java)\n- etcd is designed to be more easily scalable and is faster due to its simpler design. Preferred for contain orchestration platforms like Kubernetes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to implement consistent hashing",
    "back": "use a hash ring",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How we can adapt Djikstra's algorithm to work at the scale of Google Maps",
    "back": "- Break down the trip into smaller pieces (called routing grids)\n- For long trips, only consider arterial edges (e.g., freeways)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Tiling",
    "back": "- when we only load a tile of a map at a time\n- Saves on bandwidth and rendering time since we only have to load a small chunk of the map into the app",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Example tradeoffs of latency vs. throughput",
    "back": "- In short: higher batch sizes means higher throughput but worse (higher) latency\n- Increasing throughput by adding more servers can lead to increased latency due to request routing and data replication across servers1.\n- Optimizing server software to handle more concurrent requests can increase latency as the server uses more resources to manage the increased demand1.\n- Caching for low latency may decrease overall throughput by limiting available memory for handling other requests",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Two types of messaging models",
    "back": "- Point-to-point: A message is sent through a queue to one and only one consumer. It's deleted from the queue as soon as it's sent\n- Publish-subscribe model: A message is subscribed to a topic (a category of messages) and sent to all consumers that are subscribed to that topic",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "__ database writes are slow, while __ database writes are fast",
    "back": "random access; sequential access using RAID",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Cyclic redundancy checks (CRC)",
    "back": "- Similar to hashing, but produces a hash value (e.g., \"0xEA\") that can be used on both sides of the network transaction to validate that all the content was sent correctly\n- CRCs, including CRC-8, are computationally less complex than cryptographic hash functions like MD5 or SHA, but less resistant to collisions\n- CRCs are more expensive than checksums (which basically just sum data bytes), but are more likely at detecting a difference between two pieces of content\n- CRC-8 produces an 8-bit hash, CRC-32 produces a 32-bit hash, etc.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "When does a leader respond back to a producer acknowledging that its data has been committed?",
    "back": "- it depends, but usually when a quorum of replicas respond to the leader acknowledging that they've committed the data",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Increasing write-speed in a distributed system",
    "back": "- Add a buffer to the producer such that we can batch events together before sending them all at once to the correct partition\n- Add routing logic into the producer so that we don't have to ask some external node where to send new data",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "ACK=all vs. ACK=1 vs. ACK=0",
    "back": "- ACK=all: Wait until all replicas have acknowledged the commit before having the leader acknowledge the commit to the producer\n- ACK=1: only the leader has to commit before an acknowledgement is sent. The message will be lost if the leader goes down before the data is replicated on a replica\n- ACK=0: the producer doesn't wait for acknowledgement from the leader, which means it will never retry an event\n- ACK=all means slower write speeds in exchange for better durability",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Leaders keep track of ISRs, meaning",
    "back": "in-sync replicas, or the nodes that have committed all commits within k events from the leader (where k is configurable)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Partitioning is done in Kafka by both",
    "back": "topic and partition #, where each broker (leader and replicas) store multiple (topic, partition #) combinations",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of data delivery",
    "back": "- At-most once: messages will not be delivered more than once (consistent with ACK=0)\n- Atleast once: the producer will hold the message at the top of the queue until it receives an acknowledgement from the consumers that it was consumed successfully. Messages might be - delivered more than once, though we can use idempotency to ensure that the message doesn't affect data more than once.\n- Exactly once: the message will be sent exactly once (this isn't theoretically possible, but can be approximated for financial systems with lots of checks and balances)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How would you \"pull\" PRE metrics from a server?",
    "back": "Define a /metrics endpoint and query it occasionally. If you don't get a response, that's a sign that the endpoint is down",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Value prop of Flink",
    "back": "- Flink provides a built-in Kafka connector that enables reading from and writing to Kafka topics. This connector supports exactly-once processing semantics, ensuring data integrity during stream processing.\n- Once data is ingested from Kafka, Flink can perform various transformations, aggregations, and analyses on the data streams in real-time\n- After processing, Flink can write results back to Kafka using the FlinkKafkaProducer class. This allows for creating data pipelines where processed data is made available for consumption by other services or applications",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How many seconds in a day (for calculating QPS)?",
    "back": "~10^5",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Why go through Kafka instead of just writing directly to a database?",
    "back": "- Ability to enforce exactly-once semantics so that we don't accidentally update the same value twice\n- Failover protection given the ability to replay events and reprocess data from any point in time\n- Separation of concerns between producers and consumers, allowing for independent scaling and performance optimization`",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How to enforce exactly-once delivery",
    "back": "- Store the latest offset and/or timestamp (based on the last confirmed acknowledgement) in a cache or database so that you know what was last processed",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "PRE Metrics to track",
    "back": "Latency\nMessage queue size (too many messages means we may need to add more nodes to process what's in the broker)\nUptime",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three ways to prevent double-booking",
    "back": "- Pessimistic (locking): lock the resource before starting the commit process (e.g., two phase commit)\n- Optomistic (no locking, just application logic): right before commiting, check the version number. If the version number isn't up-to-date, then recheck that the resource is still available. Better performance than pessimistic in most cases, but worse performance when dealing with tons of simultaneous conflicts\n- Database constraints: build a constraint into the database that rolls back the commit if some condition is met (i.e., if the number of bookings exceeds the # of available rooms)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "A microservice purist would say that every microservice should have its own __",
    "back": "- database, though this can cause data consistency issues across databases (e.g., if one commit fails, we have to roll back all other commits in other databases)\n- We can overcome these data consistency issues using:\n- Two phase commit, where any failure in any database triggers a rollback in all other databases. All commits happen at once\n- Have a worker process each commit separately, then roll back other commits if any part of the chain fails. Each commit happens sequentially.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three email protocols",
    "back": "- SMTP: Simple Mail Transfer Protocol, used to send messages from one mail server to another\n- POP: Post Office Protocol, where emails are stored on a remote server until they are downloaded in their entirety to a local client, after which time they are deleted from the remote server\n- IMAP: Internet Mail Access Protocol, where data is only transferred to your local device when you click on an email and data isn't deleted from the external mail service",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Three types of storage systems",
    "back": "- Block: Present raw blocks (or volumes) of storage for management by other software\n- File: Built on top of block storage, it provides a higher-level abstraction to handle files and directories\n- Object: objects are immutable and stored as objects in a flat structure, where every object belongs to a particular bucket. Sacrifices performance for high durability and low cost and is primarily used for backup and archival.",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "How data is written in S3",
    "back": "- Files are written to a write-ahead log, rather than storing as individual files, to improve retrieval speed and storage efficiency\n- Once a log fills, it's changed to read-only and a new log is started\n- Each core writes to its own read-write log to prevent slowdown when multiple cores try to write to the same log\n- A checksum is appended to the end of each object to detect data corruption",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "A __ is fast for writes and slower for reads, while a __ is the opposite",
    "back": "SSTable (i.e., RocksDB, B+ tree)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Erasure coding",
    "back": "- An alternative to replicating data where we split a piece of data into chunks and split it across nodes randomly\n- The chunks can then be used to reconstruct a file should the node go down\n- Used for S3 storage because we don't care about read performance as much",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Skip list (used in Redis sorted sets, along with a hashmap)",
    "back": "- A data structure that uses multiple levels of indexes to quickly find values in O(logn)\n- Maintains a sorted order with O(logn) insertions\n- Easier to implement a balance than tree-based structures (like heaps)",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "An alternative to logging to disk when we care about persistence",
    "back": "use a read replica",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Unique requirements when dealing with financial systems",
    "back": "- Reconcilation: we have to be able to compare our database to some secondary source of truth\n- Replay: we want to be able to replay all transactions for auditing purposes",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "LSM",
    "back": "- log-structured merge tree (LSM) is a pattern that Cassandra and RocksDB use to enable O(1) writes, O(logn) reads\n- Stores the data in memory, and occasionally flushes it to disk in a tree format for O(logn) reads",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Event Sourcing",
    "back": "- Where we store events, instead of just current states",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Kafka Pub-Sub",
    "back": "- Messages are stored in topics and partitioned across brokers\n- Consumer groups pull messages by topic\n- Offers messaging order guarantees\n- Requires consumer groups to pull messages, vs. Redis Pub/Sub which pushes messages to all subscribers",
    "tags": [
      "system_design"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "Pre-Training vs Post-Training",
    "back": "In short:\n- Pre-training: learn general language features and patternrs\n- Post-training: tailoring the LLM to complete a specific task\n\n*Pre-training* (the part that usually entails only unsupervised learning) involves training the entire model architecture (encoder, decoder, or both) on a large corpus of unlabeled data to learn general language features and patterns. This process typically includes:\n- Random initialization of all model parameters\n- Training on vast amounts of diverse text data\n- Learning general language understanding and knowledge\n\n*Post-training* (the part that usually entails only supervised learning), which includes fine-tuning and other adaptation techniques, involves:\n- Loading parameters from the pre-trained model\n- Training on smaller, task-specific datasets\n- Adjusting model parameters to optimize performance for specific tasks/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nDoes ChatGPT use encoder-decoder architectures?",
    "back": "No, it uses decoder-only architecture\n\nGoogle Translate uses encoder-decoder architectures to translate text\n\nImage captioning services, text summarization services, and speech recognition services all use encoder-decoder architectures/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPrompting / In-Context Learning",
    "back": "Using prompt engineering to get a desired output from an LLM, rather than finetuning/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nImplementing Masked Language Modeling (MLM)",
    "back": "- Choose 15% of the words\n- Mask 80% of those words using a [MASK] token, change 10% of those words to something random, and leave the other 10% unchanged/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/n____ has been shown to hurt LLM training",
    "back": "Next Sentence Prediction\n\nsource: https://taoyds.github.io/assets/courses/COMP3361-lec11.pdf/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nExamples of encoder-only, decoder-only, and encoder-decoder architectures, and pros / cons of each",
    "back": "Encoder-only: BERT\n- Considers bi-directional context\n- Capture intricate contextual relationships\n- Fastest at making inferences since it doesn't generate sequences\n- Not good at generating open-text from left-to-right, one token at a time\n\nDecoder-only: GPT\n- Great at generative tasks\n- Doesn't learn bi-directional context or complex relationships\n- Slower than encoders at inference time since each word has to be predicted sequentially\n\nEncoder-decoder: T5\n- A nice middle ground between leveraging bidirectional contexts and open-text generation\n- Good for multi-task ne-tuning\n\n- Require more text wrangling\n- Harder to train\n- Less flexible for natural language generation\n- Slowest at making inferences/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nEvaluating LLM Performance",
    "back": "### 0. Automatic Metrics:\n- Perplexity: How well an LLM does at predicting the next word in a sequence\n- BLEU: Compare the model's output with some reference transcription\n- ROGUE: Measures similarity between summary and annotation.\n- Accuracy: Whether the model returns the expected output for a given question.\n\n### 1. Quality of Content \n- **Correctness:**: Assesses factual accuracy based on ground truth. \n- **Relevance:**: Evaluates how well the output addresses the given input. \n- **Hallucination:**: Measures the presence of fabricated or incorrect information. \n\n### 2. Linguistic Characteristics \n- **Coherence and Fluency:**: Evaluates grammatical correctness, readability, and logical consistency. \n- **Conciseness:**: Assesses how efficiently the information is conveyed. \n\n### 3. Cross-Cutting Considerations \n- **Quantitative vs. Qualitative:** - Balance between numerical scores and human judgment. \n- **Consistency:** - Assesses the model's ability to provide stable outputs across multiple runs. \n- **Safety and Responsibility:** \n- Evaluates for bias, toxicity, and ethical considerations./n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhy Transformers over RNNs?",
    "back": "- RNNs have trouble keeping track of relationships between words over large sequences (vanishing gradient problem)\n- RNNs aren't parallelizable because they have a feedback loop, whereas Transformers do not -> allows for training on significantly more data \n- The catch is that Transformers need positional encodings to understand the sequence of words, whereas RNNs learn the sequence through their loops\n\nhttps://princeton-nlp.github.io/cos484/lectures/lec13.pdf/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhy is multi-head attention important?",
    "back": "- Having multiple attention functions (i.e., multiple heads) allows the network to capture different types of relationships. One head can focus on syntactic relationships, while the other can focus on semantic meanings, during the encoding process\n- Used in encoders, whereas masked multi-head attention is used in decoders/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat is masked multi-head attention?",
    "back": "- Goal: enable parallelizable while NOT looking into the future\n- We apply a mask over the self-attention scores of all words ahead of the target word so that we don't consider relationships with words that haven't yet been written, effectively preventing the model from \"cheating\" by looking at future tokens. This is essential for autoregressive tasks where predictions are made sequentially, such as text generation or translation\n- We use masked multi-head attention in the decoder/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPros and cons of transformers",
    "back": "- Easier to capture long-range dependencies: we draw attention between every pair of words! \u2022\n- Easier to parallelize: Q = XWQ K = XW K V = XWV \n\n- Are positional encodings enough to capture positional information? Otherwise self-attention is an unordered function of its input\n- Quadratic computation in self-attention can become very slow when the sequence length is large; that's why GPT sets a max length of 1024 tokens/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nExplain Best Match 25",
    "back": "- BM25 (Best Match 25) is a popular ranking function used in information retrieval systems to estimate the relevance of documents to a given search query. \n- It improves upon the traditional TF-IDF method by addressing issues such as keyword saturation and document length normalization\n- The BM25 formula calculates a score for each document relative to a specific query, with higher scores indicating greater relevance. The score is similar to TF-IDF, except longer documents are penalized to avoid having a higher score\nexample bm25 index:\nbm25_index = { \"hello\": \n { \"doc_ids\": [1], \n \"term_frequencies\": {1: 1}, \n \"idf\": 0.693, \n\"doc_lengths\": {1: 4} }, .../n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nFull hybrid search",
    "back": "Finding the most relevant docs for a query using both BM25 and vector search, then merging and re-ranking the results\n\n1. It performs a vector search using a distance metric (typically cosine or dot product).\n2. It performs a full-text search using the BM25 scoring algorithm.\n3. It merges the results using Reciprocal Rank Fusion algorithm, which uses the summed reciprocal rank of every document across both searches to determine the top k docs.\n4. It re-ranks the results using semantic ranker, a machine learning model that compares each result to the original usery query and assigns a score from 0-4./n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nQuantization",
    "back": "- representing model parameters with lower precision (8-bit vs. 32-bit) to reduce memory footprint and increase inference speed\n- typically done after training to speed-up the inference process/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nSpectogram",
    "back": "A spectrogram is a visual representation of the spectrum of frequencies in a sound or other signal as they vary with time. It's usually depicted as a heat map or image where:\n- The horizontal axis represents time\n- The vertical axis represents frequency\n- The color or brightness intensity indicates the amplitude or energy of the frequency at each point in time\n\nFor example, in a spectrogram of speech:\n- You might see horizontal lines or bands representing sustained vowel sounds\n- Vertical lines or streaks could indicate sudden consonant sounds\n- The overall pattern would change as different words are spoken, with varying intensities across different frequency ranges\n\nYou can usually store spectograms to avoid privacy considerations that would prevent you from storing actual speech/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nIs passing user context into a prompt an example of RAG?",
    "back": "No. RAG really should involve querying for information that's useful for the user's specific request. The documents retrieved should be from authoritative, external sources, and should change depending on what the user asked.\n\nQuery matching -> looking up the correct output based on similar, annotated queries that are stored in vector search\n\nAn example of RAG -> retrieving relevant conversation history from prior conversations/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat metrics are used for measuring NLP outputs?",
    "back": "- Word Error Rate: The % of words that were correct. Usually used for ASR\n- BLEU measures how many n-grams (like words or sequences of words) in the candidate translation appear in the reference translation, focusing on precision. This is important in translation tasks where generating correct phrases and word order is essential for fluency and accuracy.\n- ROUGE evaluates the overlap of n-grams between the generated summary and the reference. It emphasizes recall, making it suitable for summarization tasks where capturing key points from the original text is critical./n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat's the difference between MIPS (Maximum Inner Product Search) vs cosine similarity?",
    "back": "- MIPS: This method is sensitive to the magnitude of the vectors, meaning that larger vectors can disproportionately influence the results. This is helpful when document length is a positive signal of relevance.\n- Cosine similarity: measures the angle between two vectors and is normalized by their magnitudes. This means it evaluates similarity based solely on direction rather than magnitude, making it useful in contexts where the length of the vector should not affect similarity scores\n\nNote that the original RAG paper used MIPS./n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nIn-Context Learning",
    "back": "Instead of fine-tuning (which requires calculating the gradient and updating parameters), we pass the following into the prompt itself:\n- Task descriptions\n- Input-output examples (demonstrations)\n- The specific query to be answered\n\nThe more examples we provide within the context window, the better the performance/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nHow does Reinforcement Learning with Human Feedback (RLHF) work?",
    "back": "- we train a separate reward model using human feedback data, where humans rank which LLM output they like better\n- the reward model is used to rank LLM outputs and return a relevance score\n- we adjust the model weights such that we maximize the odds of receiving a high relevance score/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat does \"parameters\" mean in the context of LLMs?",
    "back": "- Weights: Weights are **numerical values that define the strength of connections between neurons across different layers in the model**. In the context of LLMs, weights are primarily used in the attention mechanism and the feedforward neural networks that make up the model's architecture.\n- Biases: Biases are additional numerical values that are **added to the weighted sum of inputs before being passed through an activation function for each neuron in FF neural network layers and the multi-head attention layers**. They help to control the output of neurons and provide flexibility in the model's learning process. Biases can be thought of as a way to shift the activation function to the left or right, allowing the model to learn more complex patterns and relationships in the input data\n\nBoth weights and biases start as random coefficients which are adjusted during training to minimize the loss function/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/n__ layers often use GeLU / ReLU for their activation functions, while __ layers usually use softmax",
    "back": "FF neural networks; attention mechanisms/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nFF neural networks are fully connected, which means:",
    "back": "Each neuron in a layer is connected to all other neurons in the previous layer/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nWhat is the output layer in an LLM?",
    "back": "- A FF NN that generates the final prediction of the network\n- Uses a softmax activation function for outputting probabilities, or ReLU / GeLU for classification tasks\n- Output is the probability of producing each word or token/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nPost-training techniques to improve LLM outputs (7)",
    "back": "- In-Context Learning: Providing examples for your desired output\n- RAG: Querying a database to provide additional context to the LLM\n- Query Matching: Providing examples of prior queries to guide the LLM\n- Chain of Thought Reasoning: Asking the LLM to explain how it arrived at a result\n- RLHF: Using a seperate reward model to tune the LLM's parameters to better match human preferences\n- Direct Preference Optimization: An alternative to RLHF where we optimize the model weights directly by maximizing the odds that the LLM returns the preferred output without training a separate reward model\n- Quantization: Compressing model parameters to int16 or int8/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  },
  {
    "front": "/nProximal Policy Optimization (PPO)",
    "back": "- a reinforcement learning algorithm developed in 2017\n- samples actions according to the latest version of its stochastic policies\n- clips the probability ratio of new vs. old policies to avoid making too big of changes to its policies per turn\n- uses only the first-order derivative (the gradient) to be more efficient vs. other optimizations/n",
    "tags": [
      "computer_science"
    ],
    "last_asked": "2024-12-22",
    "next_review": "2024-12-22",
    "answers": {
      "correct": 0,
      "partial": 0,
      "incorrect": 0
    },
    "retired": false
  }
]